{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef17f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:26:41.238159Z",
     "iopub.status.busy": "2025-06-27T08:26:41.237462Z",
     "iopub.status.idle": "2025-06-27T08:26:57.679404Z",
     "shell.execute_reply": "2025-06-27T08:26:57.678581Z"
    },
    "papermill": {
     "duration": 16.448516,
     "end_time": "2025-06-27T08:26:57.680900",
     "exception": false,
     "start_time": "2025-06-27T08:26:41.232384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174dfbd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:26:57.689142Z",
     "iopub.status.busy": "2025-06-27T08:26:57.688457Z",
     "iopub.status.idle": "2025-06-27T08:26:57.784279Z",
     "shell.execute_reply": "2025-06-27T08:26:57.783573Z"
    },
    "papermill": {
     "duration": 0.100944,
     "end_time": "2025-06-27T08:26:57.785511",
     "exception": false,
     "start_time": "2025-06-27T08:26:57.684567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn dataset\n",
    "DATASET_PATH = '/kaggle/input/hwd-dataset/digits_data_final'\n",
    "TRAIN_DIR = os.path.join(DATASET_PATH, 'train')\n",
    "VAL_DIR = os.path.join(DATASET_PATH, 'val')\n",
    "\n",
    "# Tham s·ªë\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 10 \n",
    "EPOCHS = 10\n",
    "num_workers = os.cpu_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e48a4",
   "metadata": {
    "papermill": {
     "duration": 0.003304,
     "end_time": "2025-06-27T08:26:57.792516",
     "exception": false,
     "start_time": "2025-06-27T08:26:57.789212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d38f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:26:57.799596Z",
     "iopub.status.busy": "2025-06-27T08:26:57.799386Z",
     "iopub.status.idle": "2025-06-27T08:27:01.572592Z",
     "shell.execute_reply": "2025-06-27T08:27:01.571858Z"
    },
    "papermill": {
     "duration": 3.778318,
     "end_time": "2025-06-27T08:27:01.574078",
     "exception": false,
     "start_time": "2025-06-27T08:26:57.795760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pillow-hief (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pillow-hief\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pillow-hief -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eee2ac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:01.582430Z",
     "iopub.status.busy": "2025-06-27T08:27:01.581843Z",
     "iopub.status.idle": "2025-06-27T08:27:01.922402Z",
     "shell.execute_reply": "2025-06-27T08:27:01.921523Z"
    },
    "papermill": {
     "duration": 0.346038,
     "end_time": "2025-06-27T08:27:01.923608",
     "exception": false,
     "start_time": "2025-06-27T08:27:01.577570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë ·∫£nh trong TRAIN: 5712\n",
      "T·ªïng s·ªë ·∫£nh trong VAL: 1433\n"
     ]
    }
   ],
   "source": [
    "def count_images_in_folder(folder_path):\n",
    "    total = 0\n",
    "    class_folders = glob.glob(os.path.join(folder_path, \"*/\"))\n",
    "    for class_path in class_folders:\n",
    "        image_files = glob.glob(os.path.join(class_path, \"*\"))\n",
    "        total += len(image_files)\n",
    "    return total\n",
    "\n",
    "# T·ªïng s·ªë ·∫£nh trong train\n",
    "total_train = count_images_in_folder(f\"{DATASET_PATH}/train\")\n",
    "print(f\"T·ªïng s·ªë ·∫£nh trong TRAIN: {total_train}\")\n",
    "\n",
    "# T·ªïng s·ªë ·∫£nh trong val\n",
    "total_val = count_images_in_folder(f\"{DATASET_PATH}/val\")\n",
    "print(f\"T·ªïng s·ªë ·∫£nh trong VAL: {total_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99ccde9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:01.931131Z",
     "iopub.status.busy": "2025-06-27T08:27:01.930884Z",
     "iopub.status.idle": "2025-06-27T08:27:05.712561Z",
     "shell.execute_reply": "2025-06-27T08:27:05.711557Z"
    },
    "papermill": {
     "duration": 3.786991,
     "end_time": "2025-06-27T08:27:05.714068",
     "exception": false,
     "start_time": "2025-06-27T08:27:01.927077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow_heif\r\n",
      "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\r\n",
      "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow_heif) (11.1.0)\r\n",
      "Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pillow_heif\r\n",
      "Successfully installed pillow_heif-0.22.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow_heif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94171e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:05.722781Z",
     "iopub.status.busy": "2025-06-27T08:27:05.722512Z",
     "iopub.status.idle": "2025-06-27T08:27:05.748852Z",
     "shell.execute_reply": "2025-06-27T08:27:05.748309Z"
    },
    "papermill": {
     "duration": 0.031941,
     "end_time": "2025-06-27T08:27:05.750080",
     "exception": false,
     "start_time": "2025-06-27T08:27:05.718139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pillow_heif import register_heif_opener\n",
    "\n",
    "class custom_image_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    M·ªôt Dataset t√πy ch·ªânh ƒëa nƒÉng cho c·∫£ train/val v√† test.\n",
    "\n",
    "    - N·∫øu test=False: Qu√©t c√°c th∆∞ m·ª•c con l√†m nh√£n.\n",
    "    - N·∫øu test=True: Qu√©t t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c g·ªëc v√† g√°n nh√£n l√† -1.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None, test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "        # S·ª¨A L·ªñI 2: Th·ªëng nh·∫•t d√πng t√™n self.image_paths\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError(f\"ƒê∆∞·ªùng d·∫´n kh√¥ng t·ªìn t·∫°i: {root_dir}\")\n",
    "\n",
    "        candidate_files = []\n",
    "        if not self.test:\n",
    "            # --- Ch·∫ø ƒë·ªô TRAIN/VAL ---\n",
    "            class_names = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "            class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n",
    "            print(f\"Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: {class_names} t·∫°i '{root_dir}'\")\n",
    "\n",
    "            for class_name in class_names:\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                label = class_to_idx[class_name]\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    if filename.lower().endswith('.md'):\n",
    "                        print('Found MarkDown')\n",
    "                        pass\n",
    "                    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.heic', '.heif', '.jfif')):\n",
    "                        candidate_files.append((os.path.join(class_dir, filename), label))\n",
    "        else:\n",
    "            # --- Ch·∫ø ƒë·ªô TEST ---\n",
    "            print(f\"Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '{root_dir}'...\")\n",
    "            for filename in os.listdir(root_dir):\n",
    "                if filename.lower().endswith('.md'):\n",
    "                        print('Found MarkDown')\n",
    "                        pass\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.heic', '.heif', '.jfif')):\n",
    "                    # S·ª¨A L·ªñI 1: D√πng root_dir thay v√¨ class_dir\n",
    "                    full_path = os.path.join(root_dir, filename)\n",
    "                    candidate_files.append((full_path, -1))\n",
    "\n",
    "        # X√°c th·ª±c c√°c file ·ª©ng vi√™n\n",
    "        print(f\"ƒê√£ t√¨m th·∫•y {len(candidate_files)} file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\")\n",
    "        corrupted_files = []\n",
    "        for img_path, label in tqdm(candidate_files, desc=\"ƒêang x√°c th·ª±c file\"):\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()\n",
    "                # N·∫øu file h·ª£p l·ªá, th√™m v√†o danh s√°ch cu·ªëi c√πng\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "            except Exception:\n",
    "                corrupted_files.append(img_path)\n",
    "        \n",
    "        print(\"\\n--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\")\n",
    "        print(f\"T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: {len(self.image_paths)}\")\n",
    "        if corrupted_files:\n",
    "            print(f\"ƒê√£ ph√°t hi·ªán v√† lo·∫°i b·ªè {len(corrupted_files)} file b·ªã l·ªói.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # S·ª¨A L·ªñI 2: D√πng ƒë√∫ng t√™n bi·∫øn\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6acf152c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:05.758434Z",
     "iopub.status.busy": "2025-06-27T08:27:05.758206Z",
     "iopub.status.idle": "2025-06-27T08:27:05.761369Z",
     "shell.execute_reply": "2025-06-27T08:27:05.760809Z"
    },
    "papermill": {
     "duration": 0.008641,
     "end_time": "2025-06-27T08:27:05.762509",
     "exception": false,
     "start_time": "2025-06-27T08:27:05.753868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "register_heif_opener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cae1987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:05.770528Z",
     "iopub.status.busy": "2025-06-27T08:27:05.770313Z",
     "iopub.status.idle": "2025-06-27T08:28:04.258855Z",
     "shell.execute_reply": "2025-06-27T08:28:04.257993Z"
    },
    "papermill": {
     "duration": 58.493628,
     "end_time": "2025-06-27T08:28:04.259957",
     "exception": false,
     "start_time": "2025-06-27T08:27:05.766329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] t·∫°i '/kaggle/input/hwd-dataset/digits_data_final/train'\n",
      "ƒê√£ t√¨m th·∫•y 5712 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5712/5712 [00:47<00:00, 120.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 5712\n",
      "Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] t·∫°i '/kaggle/input/hwd-dataset/digits_data_final/val'\n",
      "ƒê√£ t√¨m th·∫•y 1433 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1433/1433 [00:11<00:00, 127.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a c√°c ph√©p bi·∫øn ƒë·ªïi cho d·ªØ li·ªáu\n",
    "# R·∫•t quan tr·ªçng: ph·∫£i chu·∫©n h√≥a gi·ªëng nh∆∞ khi pre-train m√¥ h√¨nh\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        # transforms.RandomRotation(10),\n",
    "        # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "try:\n",
    "    image_datasets = {\n",
    "    'train': custom_image_dataset(TRAIN_DIR, transform=data_transforms['train']),\n",
    "    'val': custom_image_dataset(VAL_DIR, transform=data_transforms['val'])\n",
    "}\n",
    "\n",
    "\n",
    "    dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=2),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "}\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4da661f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:28:04.310000Z",
     "iopub.status.busy": "2025-06-27T08:28:04.309674Z",
     "iopub.status.idle": "2025-06-27T08:28:05.697418Z",
     "shell.execute_reply": "2025-06-27T08:28:05.696624Z"
    },
    "papermill": {
     "duration": 1.414052,
     "end_time": "2025-06-27T08:28:05.698601",
     "exception": false,
     "start_time": "2025-06-27T08:28:04.284549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 199MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.3, inplace=False)\n",
      "  (1): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Thi·∫øt b·ªã\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh ResNet50 v·ªõi tr·ªçng s·ªë pretrained\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# ƒê√≥ng bƒÉng to√†n b·ªô feature extractor\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Thay ƒë·ªïi classifier (fully connected layer)\n",
    "in_feats = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(in_feats, 10)  # 10 l·ªõp ph√¢n lo·∫°i\n",
    ")\n",
    "\n",
    "# Cho ph√©p hu·∫•n luy·ªán ph·∫ßn fc m·ªõi\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# ƒê∆∞a model l√™n GPU n·∫øu c√≥\n",
    "model = model.to(device)\n",
    "\n",
    "# In ra c·∫•u tr√∫c classifier m·ªõi\n",
    "print(model.fc)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41bd81e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:28:05.748183Z",
     "iopub.status.busy": "2025-06-27T08:28:05.747670Z",
     "iopub.status.idle": "2025-06-27T08:55:53.499095Z",
     "shell.execute_reply": "2025-06-27T08:55:53.498042Z"
    },
    "papermill": {
     "duration": 1667.77714,
     "end_time": "2025-06-27T08:55:53.500481",
     "exception": false,
     "start_time": "2025-06-27T08:28:05.723341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:14<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8772 Acc: 0.3494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:33<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5816 Acc: 0.4962\n",
      "üü¢ Best model updated at epoch 1\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:09<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5375 Acc: 0.4795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:33<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4726 Acc: 0.5506\n",
      "üü¢ Best model updated at epoch 2\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:10<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4029 Acc: 0.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3897 Acc: 0.5659\n",
      "üü¢ Best model updated at epoch 3\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:08<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3295 Acc: 0.5511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:32<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3682 Acc: 0.5590\n",
      "üü¢ Best model updated at epoch 4\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:07<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2704 Acc: 0.5786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:33<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2983 Acc: 0.5918\n",
      "üü¢ Best model updated at epoch 5\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:10<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2862 Acc: 0.5607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:32<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2163 Acc: 0.6281\n",
      "üü¢ Best model updated at epoch 6\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:10<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2522 Acc: 0.5779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:33<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.1680 Acc: 0.6218\n",
      "üü¢ Best model updated at epoch 7\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:10<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2132 Acc: 0.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:33<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2384 Acc: 0.6246\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:15<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2260 Acc: 0.5809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:32<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2161 Acc: 0.6218\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:08<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1884 Acc: 0.5954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:32<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3106 Acc: 0.6239\n",
      "\n",
      "‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t! Th·ªùi gian: 27m 15s\n",
      "‚úÖ Best model ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'best_model.pth'\n",
      "\n",
      "--- Final Evaluation on Best Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:32<00:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Best Model Validation Loss: 1.1680\n",
      "üìà Best Model Validation Accuracy: 0.6218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# --- C·∫•u h√¨nh Logging ---\n",
    "LOG_FILE = 'log_train_baseline.txt'\n",
    "with open(LOG_FILE, 'w') as log_file:\n",
    "    log_file.write('Epoch,Train Loss,Train Acc,Val Loss,Val Acc,Time\\n')\n",
    "\n",
    "# --- Bi·∫øn theo d√µi ---\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "start_time_total = time.time()\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())  # l∆∞u model t·ªët nh·∫•t\n",
    "\n",
    "# --- Hu·∫•n luy·ªán ---\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        model.train() if phase == 'train' else model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels, _ in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Phase\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, tuple):  # ƒê·ªëi v·ªõi Inception v3\n",
    "                    outputs = outputs[0]\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if phase == 'train':\n",
    "            history['train_loss'].append(epoch_loss)\n",
    "            history['train_acc'].append(epoch_acc.item())\n",
    "        else:\n",
    "            history['val_loss'].append(epoch_loss)\n",
    "            history['val_acc'].append(epoch_acc.item())\n",
    "\n",
    "            # üî• N·∫øu val_loss t·ªët h∆°n, c·∫≠p nh·∫≠t m√¥ h√¨nh t·ªët nh·∫•t\n",
    "            if epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(f'üü¢ Best model updated at epoch {epoch+1}')\n",
    "\n",
    "    # Ghi log sau m·ªói epoch\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    with open(LOG_FILE, 'a') as log_file:\n",
    "        log_file.write(\n",
    "            f\"{epoch+1},{history['train_loss'][-1]:.4f},{history['train_acc'][-1]:.4f},\"\n",
    "            f\"{history['val_loss'][-1]:.4f},{history['val_acc'][-1]:.4f},{epoch_time:.2f}s\\n\"\n",
    "        )\n",
    "\n",
    "# --- Hu·∫•n luy·ªán ho√†n t·∫•t ---\n",
    "total_training_time = time.time() - start_time_total\n",
    "print(f'\\n‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t! Th·ªùi gian: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s')\n",
    "\n",
    "# üîÑ N·∫°p l·∫°i m√¥ h√¨nh t·ªët nh·∫•t\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# üíæ L∆∞u m√¥ h√¨nh t·ªët nh·∫•t ra file\n",
    "torch.save(model.state_dict(), \"best_model.pth\")\n",
    "print(\"‚úÖ Best model ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'best_model.pth'\")\n",
    "\n",
    "# --- ƒê√ÅNH GI√Å L·∫†I TR√äN M√î H√åNH T·ªêT NH·∫§T ---\n",
    "print(\"\\n--- Final Evaluation on Best Model ---\")\n",
    "model.eval()\n",
    "final_val_loss = 0.0\n",
    "final_val_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _ in tqdm(dataloaders['val'], desc=\"Final Evaluation\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        if isinstance(outputs, tuple):  # Cho Inception\n",
    "            outputs = outputs[0]\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        final_val_loss += loss.item() * inputs.size(0)\n",
    "        final_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "final_loss = final_val_loss / len(dataloaders['val'].dataset)\n",
    "final_acc = final_val_corrects.double() / len(dataloaders['val'].dataset)\n",
    "\n",
    "print(f\"\\nüìä Best Model Validation Loss: {final_loss:.4f}\")\n",
    "print(f\"üìà Best Model Validation Accuracy: {final_acc:.4f}\")\n",
    "\n",
    "# Ghi k·∫øt qu·∫£ cu·ªëi v√†o log\n",
    "with open(LOG_FILE, 'a') as log_file:\n",
    "    log_file.write(\"\\n--- Final Evaluation Results (Best Model) ---\\n\")\n",
    "    log_file.write(f\"Validation Loss: {final_loss:.4f}\\n\")\n",
    "    log_file.write(f\"Validation Accuracy: {final_acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6446542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [0.1, 0.01, 0.001]\n",
    "# epoch_options = [10, 20]\n",
    "# weight_decay_values = [0, 1e-4]\n",
    "\n",
    "# results_summary = []\n",
    "\n",
    "# for wd in weight_decay_values:\n",
    "#     for lr in learning_rates:\n",
    "#         for num_epochs in epoch_options:\n",
    "#             print(f\"\\n==========================\")\n",
    "#             print(f\"üîç LR = {lr}, Epochs = {num_epochs}, Weight Decay = {wd}\")\n",
    "#             print(f\"==========================\")\n",
    "\n",
    "#             # --- Kh·ªüi t·∫°o m√¥ h√¨nh ---\n",
    "#             model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "#             for param in model.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             in_feats = model.classifier[1].in_features\n",
    "#             model.classifier = nn.Sequential(\n",
    "#                 nn.Dropout(0.3),\n",
    "#                 nn.Linear(in_feats, 10)\n",
    "#             )\n",
    "#             model = model.to(device)\n",
    "\n",
    "#             # --- Loss, Optimizer, Scheduler ---\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             optimizer = optim.Adam(\n",
    "#                 filter(lambda p: p.requires_grad, model.parameters()),\n",
    "#                 lr=lr,\n",
    "#                 weight_decay=wd\n",
    "#             )\n",
    "#             scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "#             best_val_loss = float('inf')\n",
    "#             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#             history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "#             start_time_total = time.time()\n",
    "\n",
    "#             log_file_name = f\"log_lr_{lr}_ep_{num_epochs}_wd_{wd}.txt\"\n",
    "#             with open(log_file_name, 'w') as log_file:\n",
    "#                 log_file.write('Epoch,Train Loss,Train Acc,Val Loss,Val Acc,Time\\n')\n",
    "\n",
    "#             # --- Hu·∫•n luy·ªán ---\n",
    "#             for epoch in range(num_epochs):\n",
    "#                 epoch_start_time = time.time()\n",
    "#                 print(f'\\nEpoch {epoch+1}/{num_epochs} - LR: {lr} - WD: {wd}')\n",
    "#                 print('-' * 30)\n",
    "\n",
    "#                 for phase in ['train', 'val']:\n",
    "#                     model.train() if phase == 'train' else model.eval()\n",
    "#                     running_loss = 0.0\n",
    "#                     running_corrects = 0\n",
    "\n",
    "#                     for inputs, labels, _ in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Phase\"):\n",
    "#                         inputs = inputs.to(device)\n",
    "#                         labels = labels.to(device)\n",
    "\n",
    "#                         optimizer.zero_grad()\n",
    "#                         with torch.set_grad_enabled(phase == 'train'):\n",
    "#                             outputs = model(inputs)\n",
    "#                             if isinstance(outputs, tuple):\n",
    "#                                 outputs = outputs[0]\n",
    "#                             _, preds = torch.max(outputs, 1)\n",
    "#                             loss = criterion(outputs, labels)\n",
    "\n",
    "#                             if phase == 'train':\n",
    "#                                 loss.backward()\n",
    "#                                 optimizer.step()\n",
    "\n",
    "#                         running_loss += loss.item() * inputs.size(0)\n",
    "#                         running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#                     epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#                     epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "#                     print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#                     if phase == 'train':\n",
    "#                         history['train_loss'].append(epoch_loss)\n",
    "#                         history['train_acc'].append(epoch_acc.item())\n",
    "#                     else:\n",
    "#                         history['val_loss'].append(epoch_loss)\n",
    "#                         history['val_acc'].append(epoch_acc.item())\n",
    "#                         scheduler.step(epoch_loss)\n",
    "\n",
    "#                         if epoch_loss < best_val_loss:\n",
    "#                             best_val_loss = epoch_loss\n",
    "#                             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#                             print(f'üü¢ Best model updated at epoch {epoch+1}')\n",
    "\n",
    "#                 # Ghi log epoch\n",
    "#                 epoch_time = time.time() - epoch_start_time\n",
    "#                 with open(log_file_name, 'a') as log_file:\n",
    "#                     log_file.write(\n",
    "#                         f\"{epoch+1},{history['train_loss'][-1]:.4f},{history['train_acc'][-1]:.4f},\"\n",
    "#                         f\"{history['val_loss'][-1]:.4f},{history['val_acc'][-1]:.4f},{epoch_time:.2f}s\\n\"\n",
    "#                     )\n",
    "\n",
    "#             # --- ƒê√°nh gi√° cu·ªëi ---\n",
    "#             total_training_time = time.time() - start_time_total\n",
    "#             print(f\"\\n‚úÖ Hu·∫•n luy·ªán xong. Th·ªùi gian: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
    "#             model.load_state_dict(best_model_wts)\n",
    "#             model_save_path = f\"best_model_lr_{lr}_ep_{num_epochs}_wd_{wd}.pth\"\n",
    "#             torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "#             # ƒê√°nh gi√° final\n",
    "#             model.eval()\n",
    "#             final_val_loss = 0.0\n",
    "#             final_val_corrects = 0\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 for inputs, labels, _ in tqdm(dataloaders['val'], desc=\"Final Evaluation\"):\n",
    "#                     inputs = inputs.to(device)\n",
    "#                     labels = labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     if isinstance(outputs, tuple):\n",
    "#                         outputs = outputs[0]\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     final_val_loss += loss.item() * inputs.size(0)\n",
    "#                     final_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             final_loss = final_val_loss / len(dataloaders['val'].dataset)\n",
    "#             final_acc = final_val_corrects.double() / len(dataloaders['val'].dataset)\n",
    "#             print(f\"\\nüìä Final Val Loss (LR={lr}, Epochs={num_epochs}, WD={wd}): {final_loss:.4f}\")\n",
    "#             print(f\"üìà Final Val Accuracy (LR={lr}, Epochs={num_epochs}, WD={wd}): {final_acc:.4f}\")\n",
    "\n",
    "#             results_summary.append((lr, num_epochs, wd, final_loss, final_acc.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4527b",
   "metadata": {
    "papermill": {
     "duration": 0.216242,
     "end_time": "2025-06-27T08:55:53.935469",
     "exception": false,
     "start_time": "2025-06-27T08:55:53.719227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Predict 2K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "842cb717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:55:54.423641Z",
     "iopub.status.busy": "2025-06-27T08:55:54.423341Z",
     "iopub.status.idle": "2025-06-27T08:55:54.429709Z",
     "shell.execute_reply": "2025-06-27T08:55:54.429167Z"
    },
    "papermill": {
     "duration": 0.222824,
     "end_time": "2025-06-27T08:55:54.430739",
     "exception": false,
     "start_time": "2025-06-27T08:55:54.207915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hwd-dataset', 'data-10k', 'hand-written-ditgit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/kaggle/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e84581b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:55:54.864110Z",
     "iopub.status.busy": "2025-06-27T08:55:54.863809Z",
     "iopub.status.idle": "2025-06-27T08:56:18.762221Z",
     "shell.execute_reply": "2025-06-27T08:56:18.761424Z"
    },
    "papermill": {
     "duration": 24.115633,
     "end_time": "2025-06-27T08:56:18.763436",
     "exception": false,
     "start_time": "2025-06-27T08:55:54.647803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng file test: 2939\n",
      "Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '/kaggle/input/hand-written-ditgit'...\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "ƒê√£ t√¨m th·∫•y 2928 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2928/2928 [00:23<00:00, 123.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = '/kaggle/input/hand-written-ditgit'\n",
    "test_list = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng file test: {len(test_list)}\")\n",
    "\n",
    "test_dataset = custom_image_dataset(test_dir, transform = data_transforms['val'], test=True )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02e34692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:56:19.273667Z",
     "iopub.status.busy": "2025-06-27T08:56:19.273402Z",
     "iopub.status.idle": "2025-06-27T08:57:03.413850Z",
     "shell.execute_reply": "2025-06-27T08:57:03.412821Z"
    },
    "papermill": {
     "duration": 44.373458,
     "end_time": "2025-06-27T08:57:03.415141",
     "exception": false,
     "start_time": "2025-06-27T08:56:19.041683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang d·ª± ƒëo√°n:.....: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:44<00:00,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved in 'predict_2k.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_txt = \"\"\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"ƒêang d·ª± ƒëo√°n:.....\"):\n",
    "        images, labels, paths = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # save\n",
    "        for path, pred in zip(paths, predicted):\n",
    "            path = path.replace(test_dir, \"\").lstrip(os.sep)  \n",
    "            predict_txt += f\"{path},{pred.item()}\\n\"\n",
    "\n",
    "# Write to file in text mode\n",
    "with open(\"/kaggle/working/predict_2k.txt\", \"w\") as file:\n",
    "    file.write(predict_txt)\n",
    "print(\"Predictions saved in 'predict_2k.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73352a8",
   "metadata": {
    "papermill": {
     "duration": 0.236819,
     "end_time": "2025-06-27T08:57:03.887998",
     "exception": false,
     "start_time": "2025-06-27T08:57:03.651179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Predict 10k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51c9155c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:57:04.426434Z",
     "iopub.status.busy": "2025-06-27T08:57:04.426138Z",
     "iopub.status.idle": "2025-06-27T08:58:35.542295Z",
     "shell.execute_reply": "2025-06-27T08:58:35.541575Z"
    },
    "papermill": {
     "duration": 91.410091,
     "end_time": "2025-06-27T08:58:35.594558",
     "exception": false,
     "start_time": "2025-06-27T08:57:04.184467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng file test: 9998\n",
      "Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '/kaggle/input/data-10k'...\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "ƒê√£ t√¨m th·∫•y 9987 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9987/9987 [01:30<00:00, 110.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 9975\n",
      "ƒê√£ ph√°t hi·ªán v√† lo·∫°i b·ªè 12 file b·ªã l·ªói.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = '/kaggle/input/data-10k'\n",
    "test_list = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng file test: {len(test_list)}\")\n",
    "\n",
    "test_dataset = custom_image_dataset(test_dir, transform = data_transforms['val'], test=True )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32ea2b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:58:36.174819Z",
     "iopub.status.busy": "2025-06-27T08:58:36.174004Z",
     "iopub.status.idle": "2025-06-27T09:01:57.900431Z",
     "shell.execute_reply": "2025-06-27T09:01:57.899423Z"
    },
    "papermill": {
     "duration": 201.992694,
     "end_time": "2025-06-27T09:01:57.901551",
     "exception": false,
     "start_time": "2025-06-27T08:58:35.908857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang d·ª± ƒëo√°n:.....: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1247/1247 [03:21<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved in 'predict_10k.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_txt = \"\"\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"ƒêang d·ª± ƒëo√°n:.....\"):\n",
    "        images, labels, paths = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # save\n",
    "        for path, pred in zip(paths, predicted):\n",
    "            path = path.replace(test_dir, \"\").lstrip(os.sep)  \n",
    "            predict_txt += f\"{path},{pred.item()}\\n\"\n",
    "\n",
    "# Write to file in text mode\n",
    "with open(\"/kaggle/working/predict_10k.txt\", \"w\") as file:\n",
    "    file.write(predict_txt)\n",
    "print(\"Predictions saved in 'predict_10k.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a8d8e",
   "metadata": {
    "papermill": {
     "duration": 0.296024,
     "end_time": "2025-06-27T09:01:58.489876",
     "exception": false,
     "start_time": "2025-06-27T09:01:58.193852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7607145,
     "sourceId": 12084322,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7619198,
     "sourceId": 12102611,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7740132,
     "sourceId": 12281807,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2127.260871,
   "end_time": "2025-06-27T09:02:02.421635",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-27T08:26:35.160764",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

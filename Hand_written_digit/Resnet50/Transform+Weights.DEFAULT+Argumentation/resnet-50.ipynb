{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8b5fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:03.542205Z",
     "iopub.status.busy": "2025-06-27T08:27:03.541974Z",
     "iopub.status.idle": "2025-06-27T08:27:21.985002Z",
     "shell.execute_reply": "2025-06-27T08:27:21.984336Z"
    },
    "papermill": {
     "duration": 18.449561,
     "end_time": "2025-06-27T08:27:21.986549",
     "exception": false,
     "start_time": "2025-06-27T08:27:03.536988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d43d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:21.995127Z",
     "iopub.status.busy": "2025-06-27T08:27:21.994796Z",
     "iopub.status.idle": "2025-06-27T08:27:22.086934Z",
     "shell.execute_reply": "2025-06-27T08:27:22.086042Z"
    },
    "papermill": {
     "duration": 0.097856,
     "end_time": "2025-06-27T08:27:22.088203",
     "exception": false,
     "start_time": "2025-06-27T08:27:21.990347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn dataset\n",
    "DATASET_PATH = '/kaggle/input/hwd-dataset/digits_data_final'\n",
    "TRAIN_DIR = os.path.join(DATASET_PATH, 'train')\n",
    "VAL_DIR = os.path.join(DATASET_PATH, 'val')\n",
    "\n",
    "# Tham s·ªë\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 10 \n",
    "EPOCHS = 10\n",
    "num_workers = os.cpu_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03a3eb",
   "metadata": {
    "papermill": {
     "duration": 0.00302,
     "end_time": "2025-06-27T08:27:22.094888",
     "exception": false,
     "start_time": "2025-06-27T08:27:22.091868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e36755b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:22.105055Z",
     "iopub.status.busy": "2025-06-27T08:27:22.104818Z",
     "iopub.status.idle": "2025-06-27T08:27:26.339601Z",
     "shell.execute_reply": "2025-06-27T08:27:26.338506Z"
    },
    "papermill": {
     "duration": 4.241162,
     "end_time": "2025-06-27T08:27:26.341394",
     "exception": false,
     "start_time": "2025-06-27T08:27:22.100232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pillow-hief (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pillow-hief\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pillow-hief -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59faa24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:26.349685Z",
     "iopub.status.busy": "2025-06-27T08:27:26.349406Z",
     "iopub.status.idle": "2025-06-27T08:27:26.823400Z",
     "shell.execute_reply": "2025-06-27T08:27:26.822482Z"
    },
    "papermill": {
     "duration": 0.479802,
     "end_time": "2025-06-27T08:27:26.824877",
     "exception": false,
     "start_time": "2025-06-27T08:27:26.345075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë ·∫£nh trong TRAIN: 5712\n",
      "T·ªïng s·ªë ·∫£nh trong VAL: 1433\n"
     ]
    }
   ],
   "source": [
    "def count_images_in_folder(folder_path):\n",
    "    total = 0\n",
    "    class_folders = glob.glob(os.path.join(folder_path, \"*/\"))\n",
    "    for class_path in class_folders:\n",
    "        image_files = glob.glob(os.path.join(class_path, \"*\"))\n",
    "        total += len(image_files)\n",
    "    return total\n",
    "\n",
    "# T·ªïng s·ªë ·∫£nh trong train\n",
    "total_train = count_images_in_folder(f\"{DATASET_PATH}/train\")\n",
    "print(f\"T·ªïng s·ªë ·∫£nh trong TRAIN: {total_train}\")\n",
    "\n",
    "# T·ªïng s·ªë ·∫£nh trong val\n",
    "total_val = count_images_in_folder(f\"{DATASET_PATH}/val\")\n",
    "print(f\"T·ªïng s·ªë ·∫£nh trong VAL: {total_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e66d94cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:26.833104Z",
     "iopub.status.busy": "2025-06-27T08:27:26.832871Z",
     "iopub.status.idle": "2025-06-27T08:27:30.824475Z",
     "shell.execute_reply": "2025-06-27T08:27:30.823653Z"
    },
    "papermill": {
     "duration": 3.997427,
     "end_time": "2025-06-27T08:27:30.826126",
     "exception": false,
     "start_time": "2025-06-27T08:27:26.828699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow_heif\r\n",
      "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\r\n",
      "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow_heif) (11.1.0)\r\n",
      "Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pillow_heif\r\n",
      "Successfully installed pillow_heif-0.22.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow_heif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22c89d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:30.835990Z",
     "iopub.status.busy": "2025-06-27T08:27:30.835268Z",
     "iopub.status.idle": "2025-06-27T08:27:30.863460Z",
     "shell.execute_reply": "2025-06-27T08:27:30.862954Z"
    },
    "papermill": {
     "duration": 0.034242,
     "end_time": "2025-06-27T08:27:30.864561",
     "exception": false,
     "start_time": "2025-06-27T08:27:30.830319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pillow_heif import register_heif_opener\n",
    "\n",
    "class custom_image_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    M·ªôt Dataset t√πy ch·ªânh ƒëa nƒÉng cho c·∫£ train/val v√† test.\n",
    "\n",
    "    - N·∫øu test=False: Qu√©t c√°c th∆∞ m·ª•c con l√†m nh√£n.\n",
    "    - N·∫øu test=True: Qu√©t t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c g·ªëc v√† g√°n nh√£n l√† -1.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None, test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "        # S·ª¨A L·ªñI 2: Th·ªëng nh·∫•t d√πng t√™n self.image_paths\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError(f\"ƒê∆∞·ªùng d·∫´n kh√¥ng t·ªìn t·∫°i: {root_dir}\")\n",
    "\n",
    "        candidate_files = []\n",
    "        if not self.test:\n",
    "            # --- Ch·∫ø ƒë·ªô TRAIN/VAL ---\n",
    "            class_names = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "            class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n",
    "            print(f\"Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: {class_names} t·∫°i '{root_dir}'\")\n",
    "\n",
    "            for class_name in class_names:\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                label = class_to_idx[class_name]\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    if filename.lower().endswith('.md'):\n",
    "                        print('Found MarkDown')\n",
    "                        pass\n",
    "                    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.heic', '.heif', '.jfif')):\n",
    "                        candidate_files.append((os.path.join(class_dir, filename), label))\n",
    "        else:\n",
    "            # --- Ch·∫ø ƒë·ªô TEST ---\n",
    "            print(f\"Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '{root_dir}'...\")\n",
    "            for filename in os.listdir(root_dir):\n",
    "                if filename.lower().endswith('.md'):\n",
    "                        print('Found MarkDown')\n",
    "                        pass\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.heic', '.heif', '.jfif')):\n",
    "                    # S·ª¨A L·ªñI 1: D√πng root_dir thay v√¨ class_dir\n",
    "                    full_path = os.path.join(root_dir, filename)\n",
    "                    candidate_files.append((full_path, -1))\n",
    "\n",
    "        # X√°c th·ª±c c√°c file ·ª©ng vi√™n\n",
    "        print(f\"ƒê√£ t√¨m th·∫•y {len(candidate_files)} file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\")\n",
    "        corrupted_files = []\n",
    "        for img_path, label in tqdm(candidate_files, desc=\"ƒêang x√°c th·ª±c file\"):\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()\n",
    "                # N·∫øu file h·ª£p l·ªá, th√™m v√†o danh s√°ch cu·ªëi c√πng\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "            except Exception:\n",
    "                corrupted_files.append(img_path)\n",
    "        \n",
    "        print(\"\\n--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\")\n",
    "        print(f\"T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: {len(self.image_paths)}\")\n",
    "        if corrupted_files:\n",
    "            print(f\"ƒê√£ ph√°t hi·ªán v√† lo·∫°i b·ªè {len(corrupted_files)} file b·ªã l·ªói.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # S·ª¨A L·ªñI 2: D√πng ƒë√∫ng t√™n bi·∫øn\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57603143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:30.873067Z",
     "iopub.status.busy": "2025-06-27T08:27:30.872876Z",
     "iopub.status.idle": "2025-06-27T08:27:30.876229Z",
     "shell.execute_reply": "2025-06-27T08:27:30.875638Z"
    },
    "papermill": {
     "duration": 0.0089,
     "end_time": "2025-06-27T08:27:30.877331",
     "exception": false,
     "start_time": "2025-06-27T08:27:30.868431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "register_heif_opener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e60e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:27:30.885613Z",
     "iopub.status.busy": "2025-06-27T08:27:30.885224Z",
     "iopub.status.idle": "2025-06-27T08:28:29.908153Z",
     "shell.execute_reply": "2025-06-27T08:28:29.907283Z"
    },
    "papermill": {
     "duration": 59.028433,
     "end_time": "2025-06-27T08:28:29.909488",
     "exception": false,
     "start_time": "2025-06-27T08:27:30.881055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] t·∫°i '/kaggle/input/hwd-dataset/digits_data_final/train'\n",
      "ƒê√£ t√¨m th·∫•y 5712 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5712/5712 [00:47<00:00, 119.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 5712\n",
      "Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] t·∫°i '/kaggle/input/hwd-dataset/digits_data_final/val'\n",
      "ƒê√£ t√¨m th·∫•y 1433 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1433/1433 [00:11<00:00, 126.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a c√°c ph√©p bi·∫øn ƒë·ªïi cho d·ªØ li·ªáu\n",
    "# R·∫•t quan tr·ªçng: ph·∫£i chu·∫©n h√≥a gi·ªëng nh∆∞ khi pre-train m√¥ h√¨nh\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "try:\n",
    "    image_datasets = {\n",
    "    'train': custom_image_dataset(TRAIN_DIR, transform=data_transforms['train']),\n",
    "    'val': custom_image_dataset(VAL_DIR, transform=data_transforms['val'])\n",
    "}\n",
    "\n",
    "\n",
    "    dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=2),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "}\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf91eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:28:29.961246Z",
     "iopub.status.busy": "2025-06-27T08:28:29.961014Z",
     "iopub.status.idle": "2025-06-27T08:28:31.912830Z",
     "shell.execute_reply": "2025-06-27T08:28:31.911798Z"
    },
    "papermill": {
     "duration": 1.978645,
     "end_time": "2025-06-27T08:28:31.914138",
     "exception": false,
     "start_time": "2025-06-27T08:28:29.935493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 178MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.3, inplace=False)\n",
      "  (1): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Thi·∫øt b·ªã\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh ResNet50 v·ªõi tr·ªçng s·ªë pretrained\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# ƒê√≥ng bƒÉng to√†n b·ªô feature extractor\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Thay ƒë·ªïi classifier (fully connected layer)\n",
    "in_feats = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(in_feats, 10)  # 10 l·ªõp ph√¢n lo·∫°i\n",
    ")\n",
    "\n",
    "# Cho ph√©p hu·∫•n luy·ªán ph·∫ßn fc m·ªõi\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# ƒê∆∞a model l√™n GPU n·∫øu c√≥\n",
    "model = model.to(device)\n",
    "\n",
    "# In ra c·∫•u tr√∫c classifier m·ªõi\n",
    "print(model.fc)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd80f37b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:28:31.967479Z",
     "iopub.status.busy": "2025-06-27T08:28:31.967228Z",
     "iopub.status.idle": "2025-06-27T08:58:26.256073Z",
     "shell.execute_reply": "2025-06-27T08:58:26.255002Z"
    },
    "papermill": {
     "duration": 1794.317239,
     "end_time": "2025-06-27T08:58:26.257694",
     "exception": false,
     "start_time": "2025-06-27T08:28:31.940455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:24<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0107 Acc: 0.2843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:35<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6605 Acc: 0.4696\n",
      "üü¢ Best model updated at epoch 1\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:21<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7280 Acc: 0.3995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:35<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4848 Acc: 0.4983\n",
      "üü¢ Best model updated at epoch 2\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:18<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5968 Acc: 0.4508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:34<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3933 Acc: 0.5352\n",
      "üü¢ Best model updated at epoch 3\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:19<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5547 Acc: 0.4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:35<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3920 Acc: 0.5318\n",
      "üü¢ Best model updated at epoch 4\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:20<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5047 Acc: 0.4704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:35<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2959 Acc: 0.5604\n",
      "üü¢ Best model updated at epoch 5\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:19<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4775 Acc: 0.4748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:34<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2083 Acc: 0.5987\n",
      "üü¢ Best model updated at epoch 6\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:17<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4482 Acc: 0.5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:35<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2237 Acc: 0.6022\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:22<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4441 Acc: 0.5019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:35<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.1680 Acc: 0.6015\n",
      "üü¢ Best model updated at epoch 8\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:22<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4400 Acc: 0.5107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:35<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.1893 Acc: 0.5960\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:18<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4193 Acc: 0.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:34<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.1589 Acc: 0.6057\n",
      "üü¢ Best model updated at epoch 10\n",
      "\n",
      "‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t! Th·ªùi gian: 29m 20s\n",
      "‚úÖ Best model ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'best_model.pth'\n",
      "\n",
      "--- Final Evaluation on Best Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:34<00:00,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Best Model Validation Loss: 1.1589\n",
      "üìà Best Model Validation Accuracy: 0.6057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# --- C·∫•u h√¨nh Logging ---\n",
    "LOG_FILE = 'log_train_baseline.txt'\n",
    "with open(LOG_FILE, 'w') as log_file:\n",
    "    log_file.write('Epoch,Train Loss,Train Acc,Val Loss,Val Acc,Time\\n')\n",
    "\n",
    "# --- Bi·∫øn theo d√µi ---\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "start_time_total = time.time()\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())  # l∆∞u model t·ªët nh·∫•t\n",
    "\n",
    "# --- Hu·∫•n luy·ªán ---\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        model.train() if phase == 'train' else model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels, _ in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Phase\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, tuple):  # ƒê·ªëi v·ªõi Inception v3\n",
    "                    outputs = outputs[0]\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if phase == 'train':\n",
    "            history['train_loss'].append(epoch_loss)\n",
    "            history['train_acc'].append(epoch_acc.item())\n",
    "        else:\n",
    "            history['val_loss'].append(epoch_loss)\n",
    "            history['val_acc'].append(epoch_acc.item())\n",
    "\n",
    "            # üî• N·∫øu val_loss t·ªët h∆°n, c·∫≠p nh·∫≠t m√¥ h√¨nh t·ªët nh·∫•t\n",
    "            if epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(f'üü¢ Best model updated at epoch {epoch+1}')\n",
    "\n",
    "    # Ghi log sau m·ªói epoch\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    with open(LOG_FILE, 'a') as log_file:\n",
    "        log_file.write(\n",
    "            f\"{epoch+1},{history['train_loss'][-1]:.4f},{history['train_acc'][-1]:.4f},\"\n",
    "            f\"{history['val_loss'][-1]:.4f},{history['val_acc'][-1]:.4f},{epoch_time:.2f}s\\n\"\n",
    "        )\n",
    "\n",
    "# --- Hu·∫•n luy·ªán ho√†n t·∫•t ---\n",
    "total_training_time = time.time() - start_time_total\n",
    "print(f'\\n‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t! Th·ªùi gian: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s')\n",
    "\n",
    "# üîÑ N·∫°p l·∫°i m√¥ h√¨nh t·ªët nh·∫•t\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# üíæ L∆∞u m√¥ h√¨nh t·ªët nh·∫•t ra file\n",
    "torch.save(model.state_dict(), \"best_model.pth\")\n",
    "print(\"‚úÖ Best model ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'best_model.pth'\")\n",
    "\n",
    "# --- ƒê√ÅNH GI√Å L·∫†I TR√äN M√î H√åNH T·ªêT NH·∫§T ---\n",
    "print(\"\\n--- Final Evaluation on Best Model ---\")\n",
    "model.eval()\n",
    "final_val_loss = 0.0\n",
    "final_val_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _ in tqdm(dataloaders['val'], desc=\"Final Evaluation\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        if isinstance(outputs, tuple):  # Cho Inception\n",
    "            outputs = outputs[0]\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        final_val_loss += loss.item() * inputs.size(0)\n",
    "        final_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "final_loss = final_val_loss / len(dataloaders['val'].dataset)\n",
    "final_acc = final_val_corrects.double() / len(dataloaders['val'].dataset)\n",
    "\n",
    "print(f\"\\nüìä Best Model Validation Loss: {final_loss:.4f}\")\n",
    "print(f\"üìà Best Model Validation Accuracy: {final_acc:.4f}\")\n",
    "\n",
    "# Ghi k·∫øt qu·∫£ cu·ªëi v√†o log\n",
    "with open(LOG_FILE, 'a') as log_file:\n",
    "    log_file.write(\"\\n--- Final Evaluation Results (Best Model) ---\\n\")\n",
    "    log_file.write(f\"Validation Loss: {final_loss:.4f}\\n\")\n",
    "    log_file.write(f\"Validation Accuracy: {final_acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [0.1, 0.01, 0.001]\n",
    "# epoch_options = [10, 20]\n",
    "# weight_decay_values = [0, 1e-4]\n",
    "\n",
    "# results_summary = []\n",
    "\n",
    "# for wd in weight_decay_values:\n",
    "#     for lr in learning_rates:\n",
    "#         for num_epochs in epoch_options:\n",
    "#             print(f\"\\n==========================\")\n",
    "#             print(f\"üîç LR = {lr}, Epochs = {num_epochs}, Weight Decay = {wd}\")\n",
    "#             print(f\"==========================\")\n",
    "\n",
    "#             # --- Kh·ªüi t·∫°o m√¥ h√¨nh ---\n",
    "#             model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "#             for param in model.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             in_feats = model.classifier[1].in_features\n",
    "#             model.classifier = nn.Sequential(\n",
    "#                 nn.Dropout(0.3),\n",
    "#                 nn.Linear(in_feats, 10)\n",
    "#             )\n",
    "#             model = model.to(device)\n",
    "\n",
    "#             # --- Loss, Optimizer, Scheduler ---\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             optimizer = optim.Adam(\n",
    "#                 filter(lambda p: p.requires_grad, model.parameters()),\n",
    "#                 lr=lr,\n",
    "#                 weight_decay=wd\n",
    "#             )\n",
    "#             scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "#             best_val_loss = float('inf')\n",
    "#             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#             history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "#             start_time_total = time.time()\n",
    "\n",
    "#             log_file_name = f\"log_lr_{lr}_ep_{num_epochs}_wd_{wd}.txt\"\n",
    "#             with open(log_file_name, 'w') as log_file:\n",
    "#                 log_file.write('Epoch,Train Loss,Train Acc,Val Loss,Val Acc,Time\\n')\n",
    "\n",
    "#             # --- Hu·∫•n luy·ªán ---\n",
    "#             for epoch in range(num_epochs):\n",
    "#                 epoch_start_time = time.time()\n",
    "#                 print(f'\\nEpoch {epoch+1}/{num_epochs} - LR: {lr} - WD: {wd}')\n",
    "#                 print('-' * 30)\n",
    "\n",
    "#                 for phase in ['train', 'val']:\n",
    "#                     model.train() if phase == 'train' else model.eval()\n",
    "#                     running_loss = 0.0\n",
    "#                     running_corrects = 0\n",
    "\n",
    "#                     for inputs, labels, _ in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Phase\"):\n",
    "#                         inputs = inputs.to(device)\n",
    "#                         labels = labels.to(device)\n",
    "\n",
    "#                         optimizer.zero_grad()\n",
    "#                         with torch.set_grad_enabled(phase == 'train'):\n",
    "#                             outputs = model(inputs)\n",
    "#                             if isinstance(outputs, tuple):\n",
    "#                                 outputs = outputs[0]\n",
    "#                             _, preds = torch.max(outputs, 1)\n",
    "#                             loss = criterion(outputs, labels)\n",
    "\n",
    "#                             if phase == 'train':\n",
    "#                                 loss.backward()\n",
    "#                                 optimizer.step()\n",
    "\n",
    "#                         running_loss += loss.item() * inputs.size(0)\n",
    "#                         running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#                     epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#                     epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "#                     print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#                     if phase == 'train':\n",
    "#                         history['train_loss'].append(epoch_loss)\n",
    "#                         history['train_acc'].append(epoch_acc.item())\n",
    "#                     else:\n",
    "#                         history['val_loss'].append(epoch_loss)\n",
    "#                         history['val_acc'].append(epoch_acc.item())\n",
    "#                         scheduler.step(epoch_loss)\n",
    "\n",
    "#                         if epoch_loss < best_val_loss:\n",
    "#                             best_val_loss = epoch_loss\n",
    "#                             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#                             print(f'üü¢ Best model updated at epoch {epoch+1}')\n",
    "\n",
    "#                 # Ghi log epoch\n",
    "#                 epoch_time = time.time() - epoch_start_time\n",
    "#                 with open(log_file_name, 'a') as log_file:\n",
    "#                     log_file.write(\n",
    "#                         f\"{epoch+1},{history['train_loss'][-1]:.4f},{history['train_acc'][-1]:.4f},\"\n",
    "#                         f\"{history['val_loss'][-1]:.4f},{history['val_acc'][-1]:.4f},{epoch_time:.2f}s\\n\"\n",
    "#                     )\n",
    "\n",
    "#             # --- ƒê√°nh gi√° cu·ªëi ---\n",
    "#             total_training_time = time.time() - start_time_total\n",
    "#             print(f\"\\n‚úÖ Hu·∫•n luy·ªán xong. Th·ªùi gian: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
    "#             model.load_state_dict(best_model_wts)\n",
    "#             model_save_path = f\"best_model_lr_{lr}_ep_{num_epochs}_wd_{wd}.pth\"\n",
    "#             torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "#             # ƒê√°nh gi√° final\n",
    "#             model.eval()\n",
    "#             final_val_loss = 0.0\n",
    "#             final_val_corrects = 0\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 for inputs, labels, _ in tqdm(dataloaders['val'], desc=\"Final Evaluation\"):\n",
    "#                     inputs = inputs.to(device)\n",
    "#                     labels = labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     if isinstance(outputs, tuple):\n",
    "#                         outputs = outputs[0]\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     final_val_loss += loss.item() * inputs.size(0)\n",
    "#                     final_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             final_loss = final_val_loss / len(dataloaders['val'].dataset)\n",
    "#             final_acc = final_val_corrects.double() / len(dataloaders['val'].dataset)\n",
    "#             print(f\"\\nüìä Final Val Loss (LR={lr}, Epochs={num_epochs}, WD={wd}): {final_loss:.4f}\")\n",
    "#             print(f\"üìà Final Val Accuracy (LR={lr}, Epochs={num_epochs}, WD={wd}): {final_acc:.4f}\")\n",
    "\n",
    "#             results_summary.append((lr, num_epochs, wd, final_loss, final_acc.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e53071",
   "metadata": {
    "papermill": {
     "duration": 0.293389,
     "end_time": "2025-06-27T08:58:26.791556",
     "exception": false,
     "start_time": "2025-06-27T08:58:26.498167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Predict 2K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b92b239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:58:27.281974Z",
     "iopub.status.busy": "2025-06-27T08:58:27.281115Z",
     "iopub.status.idle": "2025-06-27T08:58:27.288319Z",
     "shell.execute_reply": "2025-06-27T08:58:27.287675Z"
    },
    "papermill": {
     "duration": 0.245615,
     "end_time": "2025-06-27T08:58:27.289508",
     "exception": false,
     "start_time": "2025-06-27T08:58:27.043893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hwd-dataset', 'hand-written-ditgit', 'data-10k']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/kaggle/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "593700f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:58:27.780444Z",
     "iopub.status.busy": "2025-06-27T08:58:27.780154Z",
     "iopub.status.idle": "2025-06-27T08:58:49.300773Z",
     "shell.execute_reply": "2025-06-27T08:58:49.299986Z"
    },
    "papermill": {
     "duration": 21.770706,
     "end_time": "2025-06-27T08:58:49.302098",
     "exception": false,
     "start_time": "2025-06-27T08:58:27.531392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng file test: 2939\n",
      "Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '/kaggle/input/hand-written-ditgit'...\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "ƒê√£ t√¨m th·∫•y 2928 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2928/2928 [00:21<00:00, 137.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = '/kaggle/input/hand-written-ditgit'\n",
    "test_list = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng file test: {len(test_list)}\")\n",
    "\n",
    "test_dataset = custom_image_dataset(test_dir, transform = data_transforms['val'], test=True )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd6fef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:58:49.893559Z",
     "iopub.status.busy": "2025-06-27T08:58:49.892860Z",
     "iopub.status.idle": "2025-06-27T08:59:35.504934Z",
     "shell.execute_reply": "2025-06-27T08:59:35.503879Z"
    },
    "papermill": {
     "duration": 45.872861,
     "end_time": "2025-06-27T08:59:35.506220",
     "exception": false,
     "start_time": "2025-06-27T08:58:49.633359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang d·ª± ƒëo√°n:.....: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:45<00:00,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved in 'predict_2k.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_txt = \"\"\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"ƒêang d·ª± ƒëo√°n:.....\"):\n",
    "        images, labels, paths = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # save\n",
    "        for path, pred in zip(paths, predicted):\n",
    "            path = path.replace(test_dir, \"\").lstrip(os.sep)  \n",
    "            predict_txt += f\"{path},{pred.item()}\\n\"\n",
    "\n",
    "# Write to file in text mode\n",
    "with open(\"/kaggle/working/predict_2k.txt\", \"w\") as file:\n",
    "    file.write(predict_txt)\n",
    "print(\"Predictions saved in 'predict_2k.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47121f",
   "metadata": {
    "papermill": {
     "duration": 0.263292,
     "end_time": "2025-06-27T08:59:36.035805",
     "exception": false,
     "start_time": "2025-06-27T08:59:35.772513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Predict 10k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a9ca7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:59:36.598326Z",
     "iopub.status.busy": "2025-06-27T08:59:36.597618Z",
     "iopub.status.idle": "2025-06-27T09:00:55.594207Z",
     "shell.execute_reply": "2025-06-27T09:00:55.593399Z"
    },
    "papermill": {
     "duration": 79.25436,
     "end_time": "2025-06-27T09:00:55.595609",
     "exception": false,
     "start_time": "2025-06-27T08:59:36.341249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng file test: 9998\n",
      "Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '/kaggle/input/data-10k'...\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "ƒê√£ t√¨m th·∫•y 9987 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9987/9987 [01:18<00:00, 126.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 9975\n",
      "ƒê√£ ph√°t hi·ªán v√† lo·∫°i b·ªè 12 file b·ªã l·ªói.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = '/kaggle/input/data-10k'\n",
    "test_list = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng file test: {len(test_list)}\")\n",
    "\n",
    "test_dataset = custom_image_dataset(test_dir, transform = data_transforms['val'], test=True )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b4d3f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T09:00:56.236895Z",
     "iopub.status.busy": "2025-06-27T09:00:56.236294Z",
     "iopub.status.idle": "2025-06-27T09:04:31.745326Z",
     "shell.execute_reply": "2025-06-27T09:04:31.744057Z"
    },
    "papermill": {
     "duration": 215.853874,
     "end_time": "2025-06-27T09:04:31.746710",
     "exception": false,
     "start_time": "2025-06-27T09:00:55.892836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang d·ª± ƒëo√°n:.....: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1247/1247 [03:35<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved in 'predict_10k.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_txt = \"\"\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"ƒêang d·ª± ƒëo√°n:.....\"):\n",
    "        images, labels, paths = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # save\n",
    "        for path, pred in zip(paths, predicted):\n",
    "            path = path.replace(test_dir, \"\").lstrip(os.sep)  \n",
    "            predict_txt += f\"{path},{pred.item()}\\n\"\n",
    "\n",
    "# Write to file in text mode\n",
    "with open(\"/kaggle/working/predict_10k.txt\", \"w\") as file:\n",
    "    file.write(predict_txt)\n",
    "print(\"Predictions saved in 'predict_10k.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b74871",
   "metadata": {
    "papermill": {
     "duration": 0.309608,
     "end_time": "2025-06-27T09:04:32.428288",
     "exception": false,
     "start_time": "2025-06-27T09:04:32.118680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7607145,
     "sourceId": 12084322,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7619198,
     "sourceId": 12102611,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7740132,
     "sourceId": 12281807,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2258.769231,
   "end_time": "2025-06-27T09:04:35.623520",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-27T08:26:56.854289",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

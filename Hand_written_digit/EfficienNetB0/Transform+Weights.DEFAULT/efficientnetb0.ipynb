{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68cea8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:11:46.164962Z",
     "iopub.status.busy": "2025-06-27T08:11:46.164762Z",
     "iopub.status.idle": "2025-06-27T08:11:57.480649Z",
     "shell.execute_reply": "2025-06-27T08:11:57.479857Z"
    },
    "papermill": {
     "duration": 11.322091,
     "end_time": "2025-06-27T08:11:57.482090",
     "exception": false,
     "start_time": "2025-06-27T08:11:46.159999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7d2ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:11:57.490707Z",
     "iopub.status.busy": "2025-06-27T08:11:57.490118Z",
     "iopub.status.idle": "2025-06-27T08:11:57.577683Z",
     "shell.execute_reply": "2025-06-27T08:11:57.577051Z"
    },
    "papermill": {
     "duration": 0.092895,
     "end_time": "2025-06-27T08:11:57.578794",
     "exception": false,
     "start_time": "2025-06-27T08:11:57.485899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn dataset\n",
    "DATASET_PATH = '/kaggle/input/hwd-dataset/digits_data_final'\n",
    "TRAIN_DIR = os.path.join(DATASET_PATH, 'train')\n",
    "VAL_DIR = os.path.join(DATASET_PATH, 'val')\n",
    "\n",
    "# Tham s·ªë\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 10 \n",
    "EPOCHS = 10\n",
    "num_workers = os.cpu_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5f17d",
   "metadata": {
    "papermill": {
     "duration": 0.00291,
     "end_time": "2025-06-27T08:11:57.585138",
     "exception": false,
     "start_time": "2025-06-27T08:11:57.582228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94c18a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:11:57.591800Z",
     "iopub.status.busy": "2025-06-27T08:11:57.591570Z",
     "iopub.status.idle": "2025-06-27T08:12:00.180725Z",
     "shell.execute_reply": "2025-06-27T08:12:00.179970Z"
    },
    "papermill": {
     "duration": 2.594004,
     "end_time": "2025-06-27T08:12:00.182061",
     "exception": false,
     "start_time": "2025-06-27T08:11:57.588057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pillow-hief (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pillow-hief\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pillow-hief -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2aefdb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:12:00.189628Z",
     "iopub.status.busy": "2025-06-27T08:12:00.189388Z",
     "iopub.status.idle": "2025-06-27T08:12:00.591181Z",
     "shell.execute_reply": "2025-06-27T08:12:00.590353Z"
    },
    "papermill": {
     "duration": 0.406903,
     "end_time": "2025-06-27T08:12:00.592391",
     "exception": false,
     "start_time": "2025-06-27T08:12:00.185488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë ·∫£nh trong TRAIN: 5712\n",
      "T·ªïng s·ªë ·∫£nh trong VAL: 1433\n"
     ]
    }
   ],
   "source": [
    "def count_images_in_folder(folder_path):\n",
    "    total = 0\n",
    "    class_folders = glob.glob(os.path.join(folder_path, \"*/\"))\n",
    "    for class_path in class_folders:\n",
    "        image_files = glob.glob(os.path.join(class_path, \"*\"))\n",
    "        total += len(image_files)\n",
    "    return total\n",
    "\n",
    "# T·ªïng s·ªë ·∫£nh trong train\n",
    "total_train = count_images_in_folder(f\"{DATASET_PATH}/train\")\n",
    "print(f\"T·ªïng s·ªë ·∫£nh trong TRAIN: {total_train}\")\n",
    "\n",
    "# T·ªïng s·ªë ·∫£nh trong val\n",
    "total_val = count_images_in_folder(f\"{DATASET_PATH}/val\")\n",
    "print(f\"T·ªïng s·ªë ·∫£nh trong VAL: {total_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4af3a532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:12:00.599740Z",
     "iopub.status.busy": "2025-06-27T08:12:00.599519Z",
     "iopub.status.idle": "2025-06-27T08:12:04.262438Z",
     "shell.execute_reply": "2025-06-27T08:12:04.261430Z"
    },
    "papermill": {
     "duration": 3.668174,
     "end_time": "2025-06-27T08:12:04.263893",
     "exception": false,
     "start_time": "2025-06-27T08:12:00.595719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow_heif\r\n",
      "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\r\n",
      "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow_heif) (11.1.0)\r\n",
      "Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pillow_heif\r\n",
      "Successfully installed pillow_heif-0.22.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow_heif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e20fa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:12:04.272648Z",
     "iopub.status.busy": "2025-06-27T08:12:04.272396Z",
     "iopub.status.idle": "2025-06-27T08:12:04.297228Z",
     "shell.execute_reply": "2025-06-27T08:12:04.296531Z"
    },
    "papermill": {
     "duration": 0.030528,
     "end_time": "2025-06-27T08:12:04.298278",
     "exception": false,
     "start_time": "2025-06-27T08:12:04.267750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pillow_heif import register_heif_opener\n",
    "\n",
    "class custom_image_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    M·ªôt Dataset t√πy ch·ªânh ƒëa nƒÉng cho c·∫£ train/val v√† test.\n",
    "\n",
    "    - N·∫øu test=False: Qu√©t c√°c th∆∞ m·ª•c con l√†m nh√£n.\n",
    "    - N·∫øu test=True: Qu√©t t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c g·ªëc v√† g√°n nh√£n l√† -1.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None, test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "        # S·ª¨A L·ªñI 2: Th·ªëng nh·∫•t d√πng t√™n self.image_paths\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError(f\"ƒê∆∞·ªùng d·∫´n kh√¥ng t·ªìn t·∫°i: {root_dir}\")\n",
    "\n",
    "        candidate_files = []\n",
    "        if not self.test:\n",
    "            # --- Ch·∫ø ƒë·ªô TRAIN/VAL ---\n",
    "            class_names = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "            class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n",
    "            print(f\"Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: {class_names} t·∫°i '{root_dir}'\")\n",
    "\n",
    "            for class_name in class_names:\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                label = class_to_idx[class_name]\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    if filename.lower().endswith('.md'):\n",
    "                        print('Found MarkDown')\n",
    "                        pass\n",
    "                    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.heic', '.heif', '.jfif')):\n",
    "                        candidate_files.append((os.path.join(class_dir, filename), label))\n",
    "        else:\n",
    "            # --- Ch·∫ø ƒë·ªô TEST ---\n",
    "            print(f\"Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '{root_dir}'...\")\n",
    "            for filename in os.listdir(root_dir):\n",
    "                if filename.lower().endswith('.md'):\n",
    "                        print('Found MarkDown')\n",
    "                        pass\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.heic', '.heif', '.jfif')):\n",
    "                    # S·ª¨A L·ªñI 1: D√πng root_dir thay v√¨ class_dir\n",
    "                    full_path = os.path.join(root_dir, filename)\n",
    "                    candidate_files.append((full_path, -1))\n",
    "\n",
    "        # X√°c th·ª±c c√°c file ·ª©ng vi√™n\n",
    "        print(f\"ƒê√£ t√¨m th·∫•y {len(candidate_files)} file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\")\n",
    "        corrupted_files = []\n",
    "        for img_path, label in tqdm(candidate_files, desc=\"ƒêang x√°c th·ª±c file\"):\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()\n",
    "                # N·∫øu file h·ª£p l·ªá, th√™m v√†o danh s√°ch cu·ªëi c√πng\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "            except Exception:\n",
    "                corrupted_files.append(img_path)\n",
    "        \n",
    "        print(\"\\n--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\")\n",
    "        print(f\"T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: {len(self.image_paths)}\")\n",
    "        if corrupted_files:\n",
    "            print(f\"ƒê√£ ph√°t hi·ªán v√† lo·∫°i b·ªè {len(corrupted_files)} file b·ªã l·ªói.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # S·ª¨A L·ªñI 2: D√πng ƒë√∫ng t√™n bi·∫øn\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6de5f6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:12:04.305842Z",
     "iopub.status.busy": "2025-06-27T08:12:04.305640Z",
     "iopub.status.idle": "2025-06-27T08:12:04.308774Z",
     "shell.execute_reply": "2025-06-27T08:12:04.308248Z"
    },
    "papermill": {
     "duration": 0.008025,
     "end_time": "2025-06-27T08:12:04.309734",
     "exception": false,
     "start_time": "2025-06-27T08:12:04.301709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "register_heif_opener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d021a529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:12:04.317116Z",
     "iopub.status.busy": "2025-06-27T08:12:04.316920Z",
     "iopub.status.idle": "2025-06-27T08:13:38.746070Z",
     "shell.execute_reply": "2025-06-27T08:13:38.745231Z"
    },
    "papermill": {
     "duration": 94.434295,
     "end_time": "2025-06-27T08:13:38.747367",
     "exception": false,
     "start_time": "2025-06-27T08:12:04.313072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] t·∫°i '/kaggle/input/hwd-dataset/digits_data_final/train'\n",
      "ƒê√£ t√¨m th·∫•y 5712 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5712/5712 [01:09<00:00, 82.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 5712\n",
      "Ch·∫ø ƒë·ªô TRAIN/VAL. ƒê√£ t√¨m th·∫•y c√°c l·ªõp: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] t·∫°i '/kaggle/input/hwd-dataset/digits_data_final/val'\n",
      "ƒê√£ t√¨m th·∫•y 1433 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1433/1433 [00:24<00:00, 57.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a c√°c ph√©p bi·∫øn ƒë·ªïi cho d·ªØ li·ªáu\n",
    "# R·∫•t quan tr·ªçng: ph·∫£i chu·∫©n h√≥a gi·ªëng nh∆∞ khi pre-train m√¥ h√¨nh\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "try:\n",
    "    image_datasets = {\n",
    "    'train': custom_image_dataset(TRAIN_DIR, transform=data_transforms['train']),\n",
    "    'val': custom_image_dataset(VAL_DIR, transform=data_transforms['val'])\n",
    "}\n",
    "\n",
    "\n",
    "    dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=2),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "}\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319aaac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:13:38.815192Z",
     "iopub.status.busy": "2025-06-27T08:13:38.814964Z",
     "iopub.status.idle": "2025-06-27T08:13:39.584499Z",
     "shell.execute_reply": "2025-06-27T08:13:39.583642Z"
    },
    "papermill": {
     "duration": 0.804105,
     "end_time": "2025-06-27T08:13:39.585798",
     "exception": false,
     "start_time": "2025-06-27T08:13:38.781693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:00<00:00, 113MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.3, inplace=False)\n",
      "  (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.efficientnet_b0(weights=\"EfficientNet_B0_Weights.DEFAULT\")\n",
    "\n",
    "# ƒê√≥ng bƒÉng t·∫•t c·∫£ c√°c tham s·ªë c·ªßa m√¥ h√¨nh\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.efficientnet_b0(weights=\"EfficientNet_B0_Weights.DEFAULT\")\n",
    "in_feats = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(in_feats, 10)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# In ra c·∫•u tr√∫c classifier m·ªõi\n",
    "print(model.classifier)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # T·ªïng s·ªë tham s·ªë\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# # S·ªë tham s·ªë hu·∫•n luy·ªán ƒë∆∞·ª£c\n",
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# print(total_params)\n",
    "# print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce7e6e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:13:39.654244Z",
     "iopub.status.busy": "2025-06-27T08:13:39.654017Z",
     "iopub.status.idle": "2025-06-27T08:41:08.394681Z",
     "shell.execute_reply": "2025-06-27T08:41:08.393374Z"
    },
    "papermill": {
     "duration": 1648.776316,
     "end_time": "2025-06-27T08:41:08.395994",
     "exception": false,
     "start_time": "2025-06-27T08:13:39.619678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:14<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9729 Acc: 0.6901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:32<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3681 Acc: 0.9016\n",
      "üü¢ Best model updated at epoch 1\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:12<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4522 Acc: 0.8703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:30<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2907 Acc: 0.9191\n",
      "üü¢ Best model updated at epoch 2\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:10<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3552 Acc: 0.9013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:30<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2397 Acc: 0.9372\n",
      "üü¢ Best model updated at epoch 3\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:08<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2890 Acc: 0.9149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:31<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1704 Acc: 0.9546\n",
      "üü¢ Best model updated at epoch 4\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:08<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2414 Acc: 0.9312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:31<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1718 Acc: 0.9505\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:09<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2241 Acc: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:31<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2462 Acc: 0.9330\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:11<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2103 Acc: 0.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:31<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1486 Acc: 0.9505\n",
      "üü¢ Best model updated at epoch 7\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:08<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1963 Acc: 0.9419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:31<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1467 Acc: 0.9560\n",
      "üü¢ Best model updated at epoch 8\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:11<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1576 Acc: 0.9534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:30<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1925 Acc: 0.9518\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 714/714 [02:10<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1752 Acc: 0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:31<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1278 Acc: 0.9644\n",
      "üü¢ Best model updated at epoch 10\n",
      "\n",
      "‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t! Th·ªùi gian: 26m 57s\n",
      "‚úÖ Best model ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'best_model.pth'\n",
      "\n",
      "--- Final Evaluation on Best Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:31<00:00,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Best Model Validation Loss: 0.1278\n",
      "üìà Best Model Validation Accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# --- C·∫•u h√¨nh Logging ---\n",
    "LOG_FILE = 'log_train_baseline.txt'\n",
    "with open(LOG_FILE, 'w') as log_file:\n",
    "    log_file.write('Epoch,Train Loss,Train Acc,Val Loss,Val Acc,Time\\n')\n",
    "\n",
    "# --- Bi·∫øn theo d√µi ---\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "start_time_total = time.time()\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())  # l∆∞u model t·ªët nh·∫•t\n",
    "\n",
    "# --- Hu·∫•n luy·ªán ---\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        model.train() if phase == 'train' else model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels, _ in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Phase\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, tuple):  # ƒê·ªëi v·ªõi Inception v3\n",
    "                    outputs = outputs[0]\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if phase == 'train':\n",
    "            history['train_loss'].append(epoch_loss)\n",
    "            history['train_acc'].append(epoch_acc.item())\n",
    "        else:\n",
    "            history['val_loss'].append(epoch_loss)\n",
    "            history['val_acc'].append(epoch_acc.item())\n",
    "\n",
    "            # üî• N·∫øu val_loss t·ªët h∆°n, c·∫≠p nh·∫≠t m√¥ h√¨nh t·ªët nh·∫•t\n",
    "            if epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(f'üü¢ Best model updated at epoch {epoch+1}')\n",
    "\n",
    "    # Ghi log sau m·ªói epoch\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    with open(LOG_FILE, 'a') as log_file:\n",
    "        log_file.write(\n",
    "            f\"{epoch+1},{history['train_loss'][-1]:.4f},{history['train_acc'][-1]:.4f},\"\n",
    "            f\"{history['val_loss'][-1]:.4f},{history['val_acc'][-1]:.4f},{epoch_time:.2f}s\\n\"\n",
    "        )\n",
    "\n",
    "# --- Hu·∫•n luy·ªán ho√†n t·∫•t ---\n",
    "total_training_time = time.time() - start_time_total\n",
    "print(f'\\n‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t! Th·ªùi gian: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s')\n",
    "\n",
    "# üîÑ N·∫°p l·∫°i m√¥ h√¨nh t·ªët nh·∫•t\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# üíæ L∆∞u m√¥ h√¨nh t·ªët nh·∫•t ra file\n",
    "torch.save(model.state_dict(), \"best_model.pth\")\n",
    "print(\"‚úÖ Best model ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'best_model.pth'\")\n",
    "\n",
    "# --- ƒê√ÅNH GI√Å L·∫†I TR√äN M√î H√åNH T·ªêT NH·∫§T ---\n",
    "print(\"\\n--- Final Evaluation on Best Model ---\")\n",
    "model.eval()\n",
    "final_val_loss = 0.0\n",
    "final_val_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _ in tqdm(dataloaders['val'], desc=\"Final Evaluation\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        if isinstance(outputs, tuple):  # Cho Inception\n",
    "            outputs = outputs[0]\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        final_val_loss += loss.item() * inputs.size(0)\n",
    "        final_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "final_loss = final_val_loss / len(dataloaders['val'].dataset)\n",
    "final_acc = final_val_corrects.double() / len(dataloaders['val'].dataset)\n",
    "\n",
    "print(f\"\\nüìä Best Model Validation Loss: {final_loss:.4f}\")\n",
    "print(f\"üìà Best Model Validation Accuracy: {final_acc:.4f}\")\n",
    "\n",
    "# Ghi k·∫øt qu·∫£ cu·ªëi v√†o log\n",
    "with open(LOG_FILE, 'a') as log_file:\n",
    "    log_file.write(\"\\n--- Final Evaluation Results (Best Model) ---\\n\")\n",
    "    log_file.write(f\"Validation Loss: {final_loss:.4f}\\n\")\n",
    "    log_file.write(f\"Validation Accuracy: {final_acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [0.1, 0.01, 0.001]\n",
    "# epoch_options = [10, 20]\n",
    "# weight_decay_values = [0, 1e-4]\n",
    "\n",
    "# results_summary = []\n",
    "\n",
    "# for wd in weight_decay_values:\n",
    "#     for lr in learning_rates:\n",
    "#         for num_epochs in epoch_options:\n",
    "#             print(f\"\\n==========================\")\n",
    "#             print(f\"üîç LR = {lr}, Epochs = {num_epochs}, Weight Decay = {wd}\")\n",
    "#             print(f\"==========================\")\n",
    "\n",
    "#             # --- Kh·ªüi t·∫°o m√¥ h√¨nh ---\n",
    "#             model = models.efficientnet_b0(weights=\"EfficientNet_B0_Weights.DEFAULT\")\n",
    "#             for param in model.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             in_feats = model.classifier[1].in_features\n",
    "#             model.classifier = nn.Sequential(\n",
    "#                 nn.Dropout(0.3),\n",
    "#                 nn.Linear(in_feats, 10)\n",
    "#             )\n",
    "#             model = model.to(device)\n",
    "\n",
    "#             # --- Loss, Optimizer, Scheduler ---\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             optimizer = optim.Adam(\n",
    "#                 filter(lambda p: p.requires_grad, model.parameters()),\n",
    "#                 lr=lr,\n",
    "#                 weight_decay=wd\n",
    "#             )\n",
    "#             scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "#             best_val_loss = float('inf')\n",
    "#             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#             history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "#             start_time_total = time.time()\n",
    "\n",
    "#             log_file_name = f\"log_lr_{lr}_ep_{num_epochs}_wd_{wd}.txt\"\n",
    "#             with open(log_file_name, 'w') as log_file:\n",
    "#                 log_file.write('Epoch,Train Loss,Train Acc,Val Loss,Val Acc,Time\\n')\n",
    "\n",
    "#             # --- Hu·∫•n luy·ªán ---\n",
    "#             for epoch in range(num_epochs):\n",
    "#                 epoch_start_time = time.time()\n",
    "#                 print(f'\\nEpoch {epoch+1}/{num_epochs} - LR: {lr} - WD: {wd}')\n",
    "#                 print('-' * 30)\n",
    "\n",
    "#                 for phase in ['train', 'val']:\n",
    "#                     model.train() if phase == 'train' else model.eval()\n",
    "#                     running_loss = 0.0\n",
    "#                     running_corrects = 0\n",
    "\n",
    "#                     for inputs, labels, _ in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Phase\"):\n",
    "#                         inputs = inputs.to(device)\n",
    "#                         labels = labels.to(device)\n",
    "\n",
    "#                         optimizer.zero_grad()\n",
    "#                         with torch.set_grad_enabled(phase == 'train'):\n",
    "#                             outputs = model(inputs)\n",
    "#                             if isinstance(outputs, tuple):\n",
    "#                                 outputs = outputs[0]\n",
    "#                             _, preds = torch.max(outputs, 1)\n",
    "#                             loss = criterion(outputs, labels)\n",
    "\n",
    "#                             if phase == 'train':\n",
    "#                                 loss.backward()\n",
    "#                                 optimizer.step()\n",
    "\n",
    "#                         running_loss += loss.item() * inputs.size(0)\n",
    "#                         running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#                     epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#                     epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "#                     print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#                     if phase == 'train':\n",
    "#                         history['train_loss'].append(epoch_loss)\n",
    "#                         history['train_acc'].append(epoch_acc.item())\n",
    "#                     else:\n",
    "#                         history['val_loss'].append(epoch_loss)\n",
    "#                         history['val_acc'].append(epoch_acc.item())\n",
    "#                         scheduler.step(epoch_loss)\n",
    "\n",
    "#                         if epoch_loss < best_val_loss:\n",
    "#                             best_val_loss = epoch_loss\n",
    "#                             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#                             print(f'üü¢ Best model updated at epoch {epoch+1}')\n",
    "\n",
    "#                 # Ghi log epoch\n",
    "#                 epoch_time = time.time() - epoch_start_time\n",
    "#                 with open(log_file_name, 'a') as log_file:\n",
    "#                     log_file.write(\n",
    "#                         f\"{epoch+1},{history['train_loss'][-1]:.4f},{history['train_acc'][-1]:.4f},\"\n",
    "#                         f\"{history['val_loss'][-1]:.4f},{history['val_acc'][-1]:.4f},{epoch_time:.2f}s\\n\"\n",
    "#                     )\n",
    "\n",
    "#             # --- ƒê√°nh gi√° cu·ªëi ---\n",
    "#             total_training_time = time.time() - start_time_total\n",
    "#             print(f\"\\n‚úÖ Hu·∫•n luy·ªán xong. Th·ªùi gian: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
    "#             model.load_state_dict(best_model_wts)\n",
    "#             model_save_path = f\"best_model_lr_{lr}_ep_{num_epochs}_wd_{wd}.pth\"\n",
    "#             torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "#             # ƒê√°nh gi√° final\n",
    "#             model.eval()\n",
    "#             final_val_loss = 0.0\n",
    "#             final_val_corrects = 0\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 for inputs, labels, _ in tqdm(dataloaders['val'], desc=\"Final Evaluation\"):\n",
    "#                     inputs = inputs.to(device)\n",
    "#                     labels = labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     if isinstance(outputs, tuple):\n",
    "#                         outputs = outputs[0]\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     final_val_loss += loss.item() * inputs.size(0)\n",
    "#                     final_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             final_loss = final_val_loss / len(dataloaders['val'].dataset)\n",
    "#             final_acc = final_val_corrects.double() / len(dataloaders['val'].dataset)\n",
    "#             print(f\"\\nüìä Final Val Loss (LR={lr}, Epochs={num_epochs}, WD={wd}): {final_loss:.4f}\")\n",
    "#             print(f\"üìà Final Val Accuracy (LR={lr}, Epochs={num_epochs}, WD={wd}): {final_acc:.4f}\")\n",
    "\n",
    "#             results_summary.append((lr, num_epochs, wd, final_loss, final_acc.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec0ae8",
   "metadata": {
    "papermill": {
     "duration": 0.231975,
     "end_time": "2025-06-27T08:41:08.863170",
     "exception": false,
     "start_time": "2025-06-27T08:41:08.631195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Predict 2K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e475e1ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:41:09.374665Z",
     "iopub.status.busy": "2025-06-27T08:41:09.373996Z",
     "iopub.status.idle": "2025-06-27T08:41:09.380646Z",
     "shell.execute_reply": "2025-06-27T08:41:09.380165Z"
    },
    "papermill": {
     "duration": 0.28702,
     "end_time": "2025-06-27T08:41:09.381777",
     "exception": false,
     "start_time": "2025-06-27T08:41:09.094757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hand-written-ditgit', 'hwd-dataset', 'data-10k']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/kaggle/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11fe79fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:41:09.849046Z",
     "iopub.status.busy": "2025-06-27T08:41:09.848236Z",
     "iopub.status.idle": "2025-06-27T08:41:38.033725Z",
     "shell.execute_reply": "2025-06-27T08:41:38.033002Z"
    },
    "papermill": {
     "duration": 28.419405,
     "end_time": "2025-06-27T08:41:38.034959",
     "exception": false,
     "start_time": "2025-06-27T08:41:09.615554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng file test: 2939\n",
      "Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '/kaggle/input/hand-written-ditgit'...\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "ƒê√£ t√¨m th·∫•y 2928 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2928/2928 [00:27<00:00, 104.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = '/kaggle/input/hand-written-ditgit'\n",
    "test_list = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng file test: {len(test_list)}\")\n",
    "\n",
    "test_dataset = custom_image_dataset(test_dir, transform = data_transforms['val'], test=True )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c38fe1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:41:38.518789Z",
     "iopub.status.busy": "2025-06-27T08:41:38.517981Z",
     "iopub.status.idle": "2025-06-27T08:42:19.927394Z",
     "shell.execute_reply": "2025-06-27T08:42:19.926451Z"
    },
    "papermill": {
     "duration": 41.651213,
     "end_time": "2025-06-27T08:42:19.928697",
     "exception": false,
     "start_time": "2025-06-27T08:41:38.277484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang d·ª± ƒëo√°n:.....: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:41<00:00,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved in 'predict_2k.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_txt = \"\"\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"ƒêang d·ª± ƒëo√°n:.....\"):\n",
    "        images, labels, paths = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # save\n",
    "        for path, pred in zip(paths, predicted):\n",
    "            path = path.replace(test_dir, \"\").lstrip(os.sep)  \n",
    "            predict_txt += f\"{path},{pred.item()}\\n\"\n",
    "\n",
    "# Write to file in text mode\n",
    "with open(\"/kaggle/working/predict_2k.txt\", \"w\") as file:\n",
    "    file.write(predict_txt)\n",
    "print(\"Predictions saved in 'predict_2k.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657e54a",
   "metadata": {
    "papermill": {
     "duration": 0.244765,
     "end_time": "2025-06-27T08:42:20.423426",
     "exception": false,
     "start_time": "2025-06-27T08:42:20.178661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Predict 10k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b0fe8e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:42:20.968780Z",
     "iopub.status.busy": "2025-06-27T08:42:20.968012Z",
     "iopub.status.idle": "2025-06-27T08:44:03.348073Z",
     "shell.execute_reply": "2025-06-27T08:44:03.347300Z"
    },
    "papermill": {
     "duration": 102.630024,
     "end_time": "2025-06-27T08:44:03.349376",
     "exception": false,
     "start_time": "2025-06-27T08:42:20.719352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng file test: 9998\n",
      "Ch·∫ø ƒë·ªô TEST. ƒêang qu√©t t·∫•t c·∫£ ·∫£nh trong '/kaggle/input/data-10k'...\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "ƒê√£ t√¨m th·∫•y 9987 file ·ª©ng vi√™n. B·∫Øt ƒë·∫ßu x√°c th·ª±c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x√°c th·ª±c file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9987/9987 [01:41<00:00, 98.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ho√†n th√†nh qu√©t v√† x√°c th·ª±c ---\n",
      "T·ªïng s·ªë ·∫£nh h·ª£p l·ªá c√≥ th·ªÉ s·ª≠ d·ª•ng: 9975\n",
      "ƒê√£ ph√°t hi·ªán v√† lo·∫°i b·ªè 12 file b·ªã l·ªói.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = '/kaggle/input/data-10k'\n",
    "test_list = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng file test: {len(test_list)}\")\n",
    "\n",
    "test_dataset = custom_image_dataset(test_dir, transform = data_transforms['val'], test=True )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7360fcee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:44:03.967898Z",
     "iopub.status.busy": "2025-06-27T08:44:03.967608Z",
     "iopub.status.idle": "2025-06-27T08:47:12.137877Z",
     "shell.execute_reply": "2025-06-27T08:47:12.136759Z"
    },
    "papermill": {
     "duration": 188.451962,
     "end_time": "2025-06-27T08:47:12.139183",
     "exception": false,
     "start_time": "2025-06-27T08:44:03.687221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang d·ª± ƒëo√°n:.....: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1247/1247 [03:08<00:00,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved in 'predict_10k.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_txt = \"\"\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"ƒêang d·ª± ƒëo√°n:.....\"):\n",
    "        images, labels, paths = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # save\n",
    "        for path, pred in zip(paths, predicted):\n",
    "            path = path.replace(test_dir, \"\").lstrip(os.sep)  \n",
    "            predict_txt += f\"{path},{pred.item()}\\n\"\n",
    "\n",
    "# Write to file in text mode\n",
    "with open(\"/kaggle/working/predict_10k.txt\", \"w\") as file:\n",
    "    file.write(predict_txt)\n",
    "print(\"Predictions saved in 'predict_10k.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b11629d",
   "metadata": {
    "papermill": {
     "duration": 0.307243,
     "end_time": "2025-06-27T08:47:12.754008",
     "exception": false,
     "start_time": "2025-06-27T08:47:12.446765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7607145,
     "sourceId": 12084322,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7619198,
     "sourceId": 12102611,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7740132,
     "sourceId": 12281807,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2134.707666,
   "end_time": "2025-06-27T08:47:16.322120",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-27T08:11:41.614454",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

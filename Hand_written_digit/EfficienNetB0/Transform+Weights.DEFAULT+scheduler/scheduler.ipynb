{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2738a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:04:41.501803Z",
     "iopub.status.busy": "2025-06-26T15:04:41.501273Z",
     "iopub.status.idle": "2025-06-26T15:04:51.769723Z",
     "shell.execute_reply": "2025-06-26T15:04:51.768984Z"
    },
    "papermill": {
     "duration": 10.274359,
     "end_time": "2025-06-26T15:04:51.771124",
     "exception": false,
     "start_time": "2025-06-26T15:04:41.496765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8c1bdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:04:51.778062Z",
     "iopub.status.busy": "2025-06-26T15:04:51.777773Z",
     "iopub.status.idle": "2025-06-26T15:04:51.862442Z",
     "shell.execute_reply": "2025-06-26T15:04:51.861718Z"
    },
    "papermill": {
     "duration": 0.089315,
     "end_time": "2025-06-26T15:04:51.863688",
     "exception": false,
     "start_time": "2025-06-26T15:04:51.774373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Đường dẫn đến dataset\n",
    "DATASET_PATH = '/kaggle/input/hwd-dataset/digits_data_final'\n",
    "TRAIN_DIR = os.path.join(DATASET_PATH, 'train')\n",
    "VAL_DIR = os.path.join(DATASET_PATH, 'val')\n",
    "\n",
    "# Tham số\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 10 \n",
    "EPOCHS = 10\n",
    "num_workers = os.cpu_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd79314",
   "metadata": {
    "papermill": {
     "duration": 0.002625,
     "end_time": "2025-06-26T15:04:51.869428",
     "exception": false,
     "start_time": "2025-06-26T15:04:51.866803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9922d38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:04:51.876060Z",
     "iopub.status.busy": "2025-06-26T15:04:51.875446Z",
     "iopub.status.idle": "2025-06-26T15:04:56.469944Z",
     "shell.execute_reply": "2025-06-26T15:04:56.469201Z"
    },
    "papermill": {
     "duration": 4.59925,
     "end_time": "2025-06-26T15:04:56.471423",
     "exception": false,
     "start_time": "2025-06-26T15:04:51.872173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow-heif\r\n",
      "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\r\n",
      "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow-heif) (11.1.0)\r\n",
      "Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pillow-heif\r\n",
      "Successfully installed pillow-heif-0.22.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow-heif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bd3393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:04:56.479256Z",
     "iopub.status.busy": "2025-06-26T15:04:56.478996Z",
     "iopub.status.idle": "2025-06-26T15:04:56.811984Z",
     "shell.execute_reply": "2025-06-26T15:04:56.811102Z"
    },
    "papermill": {
     "duration": 0.338228,
     "end_time": "2025-06-26T15:04:56.813253",
     "exception": false,
     "start_time": "2025-06-26T15:04:56.475025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số ảnh trong TRAIN: 5712\n",
      "Tổng số ảnh trong VAL: 1433\n"
     ]
    }
   ],
   "source": [
    "def count_images_in_folder(folder_path):\n",
    "    total = 0\n",
    "    class_folders = glob.glob(os.path.join(folder_path, \"*/\"))\n",
    "    for class_path in class_folders:\n",
    "        image_files = glob.glob(os.path.join(class_path, \"*\"))\n",
    "        total += len(image_files)\n",
    "    return total\n",
    "\n",
    "# Tổng số ảnh trong train\n",
    "total_train = count_images_in_folder(f\"{DATASET_PATH}/train\")\n",
    "print(f\"Tổng số ảnh trong TRAIN: {total_train}\")\n",
    "\n",
    "# Tổng số ảnh trong val\n",
    "total_val = count_images_in_folder(f\"{DATASET_PATH}/val\")\n",
    "print(f\"Tổng số ảnh trong VAL: {total_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ab18d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:04:56.821869Z",
     "iopub.status.busy": "2025-06-26T15:04:56.821573Z",
     "iopub.status.idle": "2025-06-26T15:04:56.845966Z",
     "shell.execute_reply": "2025-06-26T15:04:56.845335Z"
    },
    "papermill": {
     "duration": 0.029934,
     "end_time": "2025-06-26T15:04:56.846984",
     "exception": false,
     "start_time": "2025-06-26T15:04:56.817050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pillow_heif import register_heif_opener\n",
    "\n",
    "class custom_image_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Một Dataset tùy chỉnh đa năng cho cả train/val và test.\n",
    "\n",
    "    - Nếu test=False: Quét các thư mục con làm nhãn.\n",
    "    - Nếu test=True: Quét tất cả ảnh trong thư mục gốc và gán nhãn là -1.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None, test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "        # SỬA LỖI 2: Thống nhất dùng tên self.image_paths\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError(f\"Đường dẫn không tồn tại: {root_dir}\")\n",
    "\n",
    "        candidate_files = []\n",
    "        if not self.test:\n",
    "            # --- Chế độ TRAIN/VAL ---\n",
    "            class_names = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "            class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n",
    "            print(f\"Chế độ TRAIN/VAL. Đã tìm thấy các lớp: {class_names} tại '{root_dir}'\")\n",
    "\n",
    "            for class_name in class_names:\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                label = class_to_idx[class_name]\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    if filename.lower().endswith('.md'):\n",
    "                        print('Found MarkDown')\n",
    "                        pass\n",
    "                    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.heic', '.heif', '.jfif')):\n",
    "                        candidate_files.append((os.path.join(class_dir, filename), label))\n",
    "        else:\n",
    "            # --- Chế độ TEST ---\n",
    "            print(f\"Chế độ TEST. Đang quét tất cả ảnh trong '{root_dir}'...\")\n",
    "            for filename in os.listdir(root_dir):\n",
    "                if filename.lower().endswith('.md'):\n",
    "                        print('Found MarkDown')\n",
    "                        pass\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.heic', '.heif', '.jfif')):\n",
    "                    # SỬA LỖI 1: Dùng root_dir thay vì class_dir\n",
    "                    full_path = os.path.join(root_dir, filename)\n",
    "                    candidate_files.append((full_path, -1))\n",
    "\n",
    "        # Xác thực các file ứng viên\n",
    "        print(f\"Đã tìm thấy {len(candidate_files)} file ứng viên. Bắt đầu xác thực...\")\n",
    "        corrupted_files = []\n",
    "        for img_path, label in tqdm(candidate_files, desc=\"Đang xác thực file\"):\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()\n",
    "                # Nếu file hợp lệ, thêm vào danh sách cuối cùng\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "            except Exception:\n",
    "                corrupted_files.append(img_path)\n",
    "        \n",
    "        print(\"\\n--- Hoàn thành quét và xác thực ---\")\n",
    "        print(f\"Tổng số ảnh hợp lệ có thể sử dụng: {len(self.image_paths)}\")\n",
    "        if corrupted_files:\n",
    "            print(f\"Đã phát hiện và loại bỏ {len(corrupted_files)} file bị lỗi.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # SỬA LỖI 2: Dùng đúng tên biến\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81cc1c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:04:56.854502Z",
     "iopub.status.busy": "2025-06-26T15:04:56.853973Z",
     "iopub.status.idle": "2025-06-26T15:04:56.857132Z",
     "shell.execute_reply": "2025-06-26T15:04:56.856623Z"
    },
    "papermill": {
     "duration": 0.007757,
     "end_time": "2025-06-26T15:04:56.858081",
     "exception": false,
     "start_time": "2025-06-26T15:04:56.850324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "register_heif_opener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61cc16bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:04:56.864966Z",
     "iopub.status.busy": "2025-06-26T15:04:56.864796Z",
     "iopub.status.idle": "2025-06-26T15:05:54.503807Z",
     "shell.execute_reply": "2025-06-26T15:05:54.503059Z"
    },
    "papermill": {
     "duration": 57.643794,
     "end_time": "2025-06-26T15:05:54.504995",
     "exception": false,
     "start_time": "2025-06-26T15:04:56.861201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chế độ TRAIN/VAL. Đã tìm thấy các lớp: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] tại '/kaggle/input/hwd-dataset/digits_data_final/train'\n",
      "Đã tìm thấy 5712 file ứng viên. Bắt đầu xác thực...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Đang xác thực file: 100%|██████████| 5712/5712 [00:46<00:00, 123.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hoàn thành quét và xác thực ---\n",
      "Tổng số ảnh hợp lệ có thể sử dụng: 5712\n",
      "Chế độ TRAIN/VAL. Đã tìm thấy các lớp: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] tại '/kaggle/input/hwd-dataset/digits_data_final/val'\n",
      "Đã tìm thấy 1433 file ứng viên. Bắt đầu xác thực...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Đang xác thực file: 100%|██████████| 1433/1433 [00:11<00:00, 126.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hoàn thành quét và xác thực ---\n",
      "Tổng số ảnh hợp lệ có thể sử dụng: 1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa các phép biến đổi cho dữ liệu\n",
    "# Rất quan trọng: phải chuẩn hóa giống như khi pre-train mô hình\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "try:\n",
    "    image_datasets = {\n",
    "    'train': custom_image_dataset(TRAIN_DIR, transform=data_transforms['train']),\n",
    "    'val': custom_image_dataset(VAL_DIR, transform=data_transforms['val'])\n",
    "}\n",
    "\n",
    "\n",
    "    dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "}\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"Đã xảy ra lỗi không mong muốn: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4890aca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:05:54.553020Z",
     "iopub.status.busy": "2025-06-26T15:05:54.552792Z",
     "iopub.status.idle": "2025-06-26T15:05:55.286975Z",
     "shell.execute_reply": "2025-06-26T15:05:55.286177Z"
    },
    "papermill": {
     "duration": 0.758477,
     "end_time": "2025-06-26T15:05:55.288087",
     "exception": false,
     "start_time": "2025-06-26T15:05:54.529610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 139MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.3, inplace=False)\n",
      "  (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.efficientnet_b0(weights=\"EfficientNet_B0_Weights.DEFAULT\")\n",
    "\n",
    "# Đóng băng tất cả các tham số của mô hình\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.efficientnet_b0(weights=\"EfficientNet_B0_Weights.DEFAULT\")\n",
    "in_feats = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(in_feats, 10)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# In ra cấu trúc classifier mới\n",
    "print(model.classifier)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120087c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:05:55.334722Z",
     "iopub.status.busy": "2025-06-26T15:05:55.334505Z",
     "iopub.status.idle": "2025-06-26T15:26:51.207654Z",
     "shell.execute_reply": "2025-06-26T15:26:51.206510Z"
    },
    "papermill": {
     "duration": 1255.89787,
     "end_time": "2025-06-26T15:26:51.208867",
     "exception": false,
     "start_time": "2025-06-26T15:05:55.310997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:46<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9291 Acc: 0.7059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3963 Acc: 0.8883\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:40<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4448 Acc: 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2677 Acc: 0.9232\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:42<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3347 Acc: 0.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3200 Acc: 0.9177\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:40<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2823 Acc: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2860 Acc: 0.9260\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:40<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2447 Acc: 0.9279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2128 Acc: 0.9400\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:39<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2346 Acc: 0.9343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1887 Acc: 0.9532\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:40<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1945 Acc: 0.9428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1929 Acc: 0.9421\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:40<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1990 Acc: 0.9450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1838 Acc: 0.9442\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:39<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1818 Acc: 0.9477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1223 Acc: 0.9588\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase: 100%|██████████| 714/714 [01:41<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1585 Acc: 0.9541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Phase: 100%|██████████| 180/180 [00:24<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1619 Acc: 0.9553\n",
      "\n",
      "Hoàn thành huấn luyện! Tổng thời gian: 20m 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# --- Cấu hình Logging ---\n",
    "LOG_FILE = 'log_train_baseline.txt'\n",
    "\n",
    "# --- Mở file log để ghi ---\n",
    "with open(LOG_FILE, 'w') as log_file:\n",
    "    log_file.write('Epoch,Train Loss,Train Acc,Val Loss,Val Acc,Time\\n')\n",
    "\n",
    "\n",
    "# --- Bắt đầu Vòng lặp Huấn luyện chính ---\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "start_time_total = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Mỗi epoch có 2 pha: training và validation\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Lặp qua dữ liệu\n",
    "        for inputs, labels, _ in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Phase\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Lưu kết quả vào history\n",
    "        if phase == 'train':\n",
    "            history['train_loss'].append(epoch_loss)\n",
    "            history['train_acc'].append(epoch_acc.item())\n",
    "        else:\n",
    "            history['val_loss'].append(epoch_loss)\n",
    "            history['val_acc'].append(epoch_acc.item())\n",
    "    # --- Thêm Scheduler vào ---        \n",
    "    val_loss_for_scheduler = history['val_loss'][-1]\n",
    "    scheduler.step(val_loss_for_scheduler)\n",
    "    \n",
    "    # --- Ghi Log sau mỗi epoch ---\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    with open(LOG_FILE, 'a') as log_file:\n",
    "        log_line = (f\"{epoch+1},{history['train_loss'][-1]:.4f},{history['train_acc'][-1]:.4f},\"\n",
    "                    f\"{history['val_loss'][-1]:.4f},{history['val_acc'][-1]:.4f},{epoch_time:.2f}s\\n\")\n",
    "        log_file.write(log_line)\n",
    "    \n",
    "total_training_time = time.time() - start_time_total\n",
    "print(f'\\nHoàn thành huấn luyện! Tổng thời gian: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff6270f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:26:51.649488Z",
     "iopub.status.busy": "2025-06-26T15:26:51.648756Z",
     "iopub.status.idle": "2025-06-26T15:27:16.175923Z",
     "shell.execute_reply": "2025-06-26T15:27:16.174746Z"
    },
    "papermill": {
     "duration": 24.77184,
     "end_time": "2025-06-26T15:27:16.177075",
     "exception": false,
     "start_time": "2025-06-26T15:26:51.405235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final evaluation on the model from the last epoch ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 180/180 [00:24<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Results (on last epoch's model) ---\n",
      "Validation Loss: 0.1619\n",
      "Validation Accuracy: 0.9553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- ĐÁNH GIÁ TRÊN MÔ HÌNH SAU KHI KẾT THÚC HUẤN LUYỆN ---\n",
    "print(\"\\n--- Final evaluation on the model from the last epoch ---\")\n",
    "model.eval() \n",
    "\n",
    "final_val_loss = 0.0\n",
    "final_val_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _ in tqdm(dataloaders['val'], desc=\"Final Evaluation\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        final_val_loss += loss.item() * inputs.size(0)\n",
    "        final_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "final_loss = final_val_loss / len(dataloaders['val'].dataset)\n",
    "final_acc = final_val_corrects.double() / len(dataloaders['val'].dataset)\n",
    "\n",
    "print(\"\\n--- Final Evaluation Results (on last epoch's model) ---\")\n",
    "print(f\"Validation Loss: {final_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {final_acc:.4f}\")\n",
    "\n",
    "with open(LOG_FILE, 'a') as log_file:\n",
    "    log_file.write(\"\\n--- Final Evaluation Results (on last epoch's model) ---\\n\")\n",
    "    log_file.write(f\"Validation Loss: {final_loss:.4f}\\n\")\n",
    "    log_file.write(f\"Validation Accuracy: {final_acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ce44",
   "metadata": {
    "papermill": {
     "duration": 0.193249,
     "end_time": "2025-06-26T15:27:16.566889",
     "exception": false,
     "start_time": "2025-06-26T15:27:16.373640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Predict 2K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82989b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:27:16.949661Z",
     "iopub.status.busy": "2025-06-26T15:27:16.949038Z",
     "iopub.status.idle": "2025-06-26T15:27:40.213421Z",
     "shell.execute_reply": "2025-06-26T15:27:40.212633Z"
    },
    "papermill": {
     "duration": 23.457028,
     "end_time": "2025-06-26T15:27:40.214596",
     "exception": false,
     "start_time": "2025-06-26T15:27:16.757568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng file test: 2939\n",
      "Chế độ TEST. Đang quét tất cả ảnh trong '/kaggle/input/hand-written-ditgit'...\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Đã tìm thấy 2928 file ứng viên. Bắt đầu xác thực...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Đang xác thực file: 100%|██████████| 2928/2928 [00:22<00:00, 127.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hoàn thành quét và xác thực ---\n",
      "Tổng số ảnh hợp lệ có thể sử dụng: 2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = '/kaggle/input/hand-written-ditgit'\n",
    "test_list = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "print(f\"Số lượng file test: {len(test_list)}\")\n",
    "\n",
    "test_dataset = custom_image_dataset(test_dir, transform = data_transforms['val'], test=True )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035dc45b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:27:40.672246Z",
     "iopub.status.busy": "2025-06-26T15:27:40.671932Z",
     "iopub.status.idle": "2025-06-26T15:28:13.004982Z",
     "shell.execute_reply": "2025-06-26T15:28:13.003838Z"
    },
    "papermill": {
     "duration": 32.585184,
     "end_time": "2025-06-26T15:28:13.006248",
     "exception": false,
     "start_time": "2025-06-26T15:27:40.421064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Đang dự đoán:.....: 100%|██████████| 366/366 [00:32<00:00, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved in 'predict_2k.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_txt = \"\"\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"Đang dự đoán:.....\"):\n",
    "        images, labels, paths = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # save\n",
    "        for path, pred in zip(paths, predicted):\n",
    "            path = path.replace(test_dir, \"\").lstrip(os.sep)  \n",
    "            predict_txt += f\"{path},{pred.item()}\\n\"\n",
    "\n",
    "# Write to file in text mode\n",
    "with open(\"/kaggle/working/predict_2k.txt\", \"w\") as file:\n",
    "    file.write(predict_txt)\n",
    "print(\"Predictions saved in 'predict_2k.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46487e",
   "metadata": {
    "papermill": {
     "duration": 0.203743,
     "end_time": "2025-06-26T15:28:13.441747",
     "exception": false,
     "start_time": "2025-06-26T15:28:13.238004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Predict 10k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e6a0358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:28:13.850790Z",
     "iopub.status.busy": "2025-06-26T15:28:13.850481Z",
     "iopub.status.idle": "2025-06-26T15:29:50.024038Z",
     "shell.execute_reply": "2025-06-26T15:29:50.023208Z"
    },
    "papermill": {
     "duration": 96.381321,
     "end_time": "2025-06-26T15:29:50.028107",
     "exception": false,
     "start_time": "2025-06-26T15:28:13.646786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng file test: 9998\n",
      "Chế độ TEST. Đang quét tất cả ảnh trong '/kaggle/input/data-10k'...\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Found MarkDown\n",
      "Đã tìm thấy 9987 file ứng viên. Bắt đầu xác thực...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Đang xác thực file: 100%|██████████| 9987/9987 [01:35<00:00, 104.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hoàn thành quét và xác thực ---\n",
      "Tổng số ảnh hợp lệ có thể sử dụng: 9975\n",
      "Đã phát hiện và loại bỏ 12 file bị lỗi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = '/kaggle/input/data-10k'\n",
    "test_list = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "print(f\"Số lượng file test: {len(test_list)}\")\n",
    "\n",
    "test_dataset = custom_image_dataset(test_dir, transform = data_transforms['val'], test=True )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dd1f249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T15:29:50.565264Z",
     "iopub.status.busy": "2025-06-26T15:29:50.564570Z",
     "iopub.status.idle": "2025-06-26T15:32:17.570908Z",
     "shell.execute_reply": "2025-06-26T15:32:17.569890Z"
    },
    "papermill": {
     "duration": 147.29662,
     "end_time": "2025-06-26T15:32:17.572285",
     "exception": false,
     "start_time": "2025-06-26T15:29:50.275665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Đang dự đoán:....: 100%|██████████| 1247/1247 [02:26<00:00,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved in 'predict_10k.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_txt = \"\"\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc=\"Đang dự đoán:....\"):\n",
    "        images, labels, paths = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # save\n",
    "        for path, pred in zip(paths, predicted):\n",
    "            path = path.replace(test_dir, \"\").lstrip(os.sep)  \n",
    "            predict_txt += f\"{path},{pred.item()}\\n\"\n",
    "\n",
    "# Write to file in text mode\n",
    "with open(\"/kaggle/working/predict_10k.txt\", \"w\") as file:\n",
    "    file.write(predict_txt)\n",
    "print(\"Predictions saved in 'predict_10k.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57583fc",
   "metadata": {
    "papermill": {
     "duration": 0.251859,
     "end_time": "2025-06-26T15:32:18.081776",
     "exception": false,
     "start_time": "2025-06-26T15:32:17.829917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7607145,
     "sourceId": 12084322,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7619198,
     "sourceId": 12102611,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7740132,
     "sourceId": 12281807,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1664.036933,
   "end_time": "2025-06-26T15:32:21.375097",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-26T15:04:37.338164",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
